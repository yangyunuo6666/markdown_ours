[toc]
*** 

# 可爱的小蜘蛛
## 1.	编写爬虫的一般流程：
   查看Headers选项卡，查看请求需要用到的请求头、请求参数等。然后查看Response 返回的网页结构，在看要解析的节点，有时也可以直接查看Elements选项卡，但是对于JavaScript 动态生成的网页，还是得查看Response选项卡返回的内容。
## VirtualEnv虚拟环境
+ 目的：让Python项目具有独立的环境互相不干扰，Python2和3可共存。
+ 安装：pip install virtualenv
+ 初始化与激活： 
## 2.	库的安装，终端中输入 pip install 库名（eg：pip install requests）或在设置->项目     —>Python解释器 添加。
## 3.	 网页请求方式：
- GET（可用url传递参数），获取查询信息，参数在URL中，只需一次发送和返回响应速度快。
对应抓取方式：import requests #导入requests库
url='http://www.******.cn/"  #网址，URL分为基础部分不可变和参数部分可变。-
strhtml=requests.get(url)  # 按GET方式利用requests.get（）函数获取网页数据，strhtml变量此时是一个URL对象代表整个网页，strhtml.txt表示网页源码。
print(strhtml.text)  #输出
- URL中域名带？表示URL带有请求参数后有参数名=number，http://www.runoob.com/s?wd=python3可自己设定，如：
  * url=http://www.runoob.com/s
  * params={"wd":"python"}
  * r=requests.get(url,params=params)#动态设置。
  * URL = 'http://www.cxbz958.org/%s/%d.html' % (num,str1)可动态设置URL。
- POST，参数（date=）在request body(实体) 中，可发送的请求信息多（字典，元组，列表，JOSN等）。
开发者模式->Network选项卡->“XHR”(相关数据)->单击Headers（（requestas Method）找到数据请求方式）,j将Headers中的request URL复制并且赋给url,
- POST参数在实体中，要将请求的参数fromdate放在一个新字典(如data)中，Response = requests.post(url,date=from_data).
- Import json
Content = json.loads(response.text)#将josn格式的字符串转换为字典。
## 4.	网页解析
1.	安装bs4库（因为BeautfulSoup已经内置在bs4中了，导入时from bs4 import BeautfulSoup），安装lxml库（Python内置了HTML库但是lxml库功能强大。）
2.	BeautfulSoup解析数据：suop = BeautfulSoup(strhtml.txt,’lxml’)#使用lxml库解析数据，将解析后的数据保存在新变量soup中。
3.	数据定位：
- 找到对应路径：右击网页需要对应的数据->检查->右击高亮的对应代码->选择copy（复制） ->copy selector(复制路径)
- 使用 soup.select: data=soup.select(‘路径’)
4.	数据定位后得到了目标HTML代码,要马上保存原始数据，后可以for循环提取数据
   For item in data  :
	Result={
		‘title’:item.get_text() 提取标题。标题在<"a">标签中，提取正文用get_text()
		‘link’:item.get(‘href’) 提取链接，提取标签中的href属性用get()方法。
           } 
5.	**正则表达式**：每一篇文章都有一个数字ID可用正则表达式提取，**以单引号括起来**。
- **正则表达式符号**：\d匹配数字 +匹配前一个数字一次或多次。
  * 单一个点，匹配除‘\n’外任意一个字符。
  *  $匹配$之前的字符或模式结束的字符。
  *  \f匹配换页符，\n匹配换行符,\r匹配回车符。
  * \d<=>[0-9]匹配一个数字，\D<=>[^0-9]匹配一个非数字
  * \s匹配空白字符， \S匹配非空白字符
  * \w<=>[A-Za-z0-9]匹配任意单词字符，\W匹配任意非单词字符。eg：'\w\w\d'可匹配‘py3’
  * ^匹配开始位置，多行模式下匹配到每一行开始。$匹配结束位置，多行模式下匹配到每一行结束
  *  **\^表示开头(^\d表示必须以数字开头)，$表示结尾(\d\$表示必须与数字结尾)。|表示或(A|a匹配A,a),后接{1,19}则是限制了变量长度为1到19，正常是贪婪匹配（匹配尽可能多），末尾加？可变为非贪婪匹配** ***后接加号匹配至少由一个数字、字母、下划线组成的字符串，后接乘号匹配由字母或下化线开头后接任意个数字组成的字符串***
- Eg：‘ID’:re.findall(‘\d+’,item.get(‘href’))//fingdall(正则表达式,提取的文本)
- {a}匹配前一个字符a次。{m,n}匹配前一个字符m到n次 {m,n}?匹配前一个字符m到n次取尽可能少的情况
- \\对特殊字符转义，[abc]字符集
- (?=abc)从左至右匹配到abc停止保留abc前面的部分，(?!=abc)与前者相反。
- 常用匹配函数
  * search(pattern正则表达式，str，flags标志位)//标志位通常写为**re.I****(I(忽略大小写) M(开启多行模式) S(点可匹配换行符) L(字符集本地化) U(使预定字符集（如\d）取决于Unicode定义的字符属性 ) X(详细模式，正则表达式可以是多行的忽略空白字符并可加入注释) M|I表示两个都要 )**，匹配好后用group(N)获得第N个正则表达式的值无参数为所有的正则表达式值。
  * **match**同上，但是search匹配整个字符串直到找到，match如果开头不匹配则匹配失败。
  * **findall**参数同上，但返回是列表。1.当正则表达式有多个括号，返回的列表元素为多个字符串组成的元组。2.只有一个括号，返回列表元素为字符串。3.无括号返回为整个正则表达式匹配的内容。
  * **re.sub(pattern正则表达式，repl用于替换的字符串，str被替换的字符串，count替换的次数，flags标志位同上)**
6.**多线程**下载：
1. 一个任务就是一个进程，一个进程中的子任务就是线程。Cup分配资源给进程，线程共享资源，真正在CPU上运行的是线程。
2. 普通下载：以wd创建并打开文件，以f.write(r.content)写入（r.content二进制数据流）
3. 用-thread创建进程：
-  调用_thread 模块中的start_new_thread（）函数来产生新线程。
其语法格式如下：thread.start_new_thread(function,arge[,kwarge])
参数含义如下。
function：线程运行函数。
args：传递给线程函数的参数，使用空的元组来调用函数表示不传递任何参数，必须是元组类型。
kwargs：可选参数。
-	Threading 类常用的方法
方法	说明
run()	线程的入口点
start()	启动线程，通过调用run（）方法启动一个线程
join([time])	堵塞进程直到线程执行完毕，参数time指定超时间（s），超过指定时间就不再堵塞进程
isAlive()	检查线程是否活动
getName()	返回线程名
setName()	设置线程名

start new threadO函数创建一个线程并运行指定函数，当函数返回时，线程自动结束
3.用Thread创建进程：
-  _thread 是低级模块；threading 是高级模块，其对thread模块进行了封装，为线程提供了更强大的高级支持。绝大多数情况下只需要使用threading高级模块，threading提供了守护进程功能。
threading 模块提供了Thread 类来创建和处理线程，其有两种使用方法：直接传入要运行的方法，或从Thread 继承并覆盖run()函数。
Thread类创建线程的语法格式如下：
线程对象=threading.Thread(target=线程函数，args=（参数列表），name=线程名，group=线程组）线程名和线程组都可以省略。
- Threading 模块提供的其他方法
其他方法	说明
threading.curremtThread()	返回当前线程的变量
threading.enumerate()	返回一个包含正在运行的线程目录
threading.activeCount()	返回正在运行的进程数
**锁与同步**当一个线程1要访问共享数据时必须获得锁定，如果有别的线性已经获得锁定了，线程1就暂停称为同步阻塞。多进程稳定占资源，多线程易崩溃。



## 5.	反爬机制
1.	服务器通过识别keywor是否在Request Headers下的User-Agent中。
1.构造请求头的参数，伪装为浏览器访问。（request函数中有headers参数）
Eg：headers = {User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'<br>
'AppleWebKit/537.36 (KHTML, like Gecko)'<br>
'Chrome/63.0.3239.84 Safari/537.36',//浏览器信息<br>
'Referer':'http://www.baidu.com',//请求的URL<br>
'Connection': 'keep-alive']//http请求特点<br>
novel_req = urlib.request. Request(novel_url, headers=headers)<br>

1.  Accept：text/html,image/*（浏览器可以接收的类型)
- <em>Accept-Charset：ISO-8859-1（浏览器可以接收的编码类型）
-   Accept-Encoding:gzip,compress（浏览器可以接收压缩编码类型）
-  Accept-Language：en-us,zh-cn（浏览器可以接收的语言和国家类型）。
-   Host：请求的主机地址和端口。
- If-Modifed-Since: Tue, 11 Jul 200018：23：51 GMT（某个页面的缓存时间）。
-  Referer：请求来自于哪个页面的URL。
- User-Agent: Mozilla/4.0(compatible, MSIE 5.5， Windows NT 5.0，浏览器相关信息）。
- Cookie：浏览器暂存服务器发送的信息。
- Connection: close(1.0)/Keep-Alive(1.1)（HTTP请求版本的特点）。
- Date:Tue,11 Jul 200018:23：51GMT（请求网站的时间）。</em>

1. 通过访问频率（过快或间隔时间相同）
    -  利用time.sleep(number)增设延时，numbe= roundom.randint(0,10)秒。
    - 利用代理ip(透明代理即知你用了代理IP又知道你的真实IP，普通代理（仅知用了代理IP），高匿代理)访问，建立代理ip池，response = requests.get(url,proxies=proxies)
2. cookie限制（要先登录）：
     * 登录后抓包找到cookie字段，如访问频繁可建立一个cookie池，随机取用。
     * 用字典保存用户登录所需用户名和密码等，请求时添加到date参数中。
     * <em>Domain：域，表示当前Cookies属于哪个域或子域下面。
     * Path：表示Cookies的所属路径。
     * Expire Time/Max-Age：表示Cookies的有效期。
     * Secure：表示该Cookies只能用HTTPS传输。
     * Httponly：表示此Cookies 必须用HTTP或HTTPS传输。
     * HasKeys：通过该值指示Cookie是否含有子键，返回一个bool值。
     * Name：表示Cookie的名称。
     * Value：单个Cookie的值。
     * Values：单个Cookie所包含的键值对的集合</em>
3. JavaScript反爬：
- 将重要信息放在JavaScript中浏览器会执行JavaScript代码使的信息正常展示，而爬虫不能做到。（如Ajax动态加载数据）
- 通过抓包分析接口数据，模拟请求来获取数据。
4.  证书验证
  - 设置请求参数verify=Falae即可关闭证书验证，默认是True。
  - 默认开启状态下，verify=证书保存绝对路径。可设置使用的证书。

# 邮件发送
1.  smtplib和email模块发送，POP3接收和下载文件。 


# **scrapy框架**
+ 框架：减少了大量的重复性工作。
## 框架
[图示](https://img2018.cnblogs.com/blog/1730057/201912/1730057-20191215180753182-134919908.png)
## 创建项目
+ 
  - scrapy startproject project_name
  - cd project_name
  - scrapy genspider spider_name spider.com(指定spider名和IP种子)
+  scrapy startproject project_name
   - 命令执行后会创建多个文件
     * scrapy.cf//项目的根目录
     * settings.py// 配置文件
     * spider.py//爬虫 
## **items.py文件**，
    Item数据容器，属于接口类。
   - 使用class定义语法和Field对象声明，如下。
```
import Scarpy

class Product(scarpy.item):
  name = scarpy.Field()
  price = scarpy.Field()
  stock = scarpy.Field()
  last_update = scarpy.Field(serializer=str)
```
  其中的Field函数：是模块中的一个工具，用于定义类中的属性。当你使用 field 函数时，它允许你为字段指定默认值以及其他一些属性。未定义时自动设为默认值。
## spider.py蜘蛛文件
  - spider.py
```
import scrapy

class FirstSpider(scrapy.Spider):
    name = "spider_name"//设置蜘蛛名
    allowed_domains = ["spider.com"]//爬取域
    start_urls = ["https://spider.com"]//IP种子，可有多个。

    def parse(self, response)://解析response函数，多态
        pass
```
  - parse函数实现示例
```
  def parse(self,response):
    response = BeautfiulSoup(response,'lxml')
    for item in response.find_all('item'):
      feed_item = FeedItem()
      feed_item['title']=item.title//数据的拼接
      feed_item['link']=item.link
      yield feed_item
```

## pipelines.py管道

### **定义**：
处理item对象，必须返回一个item或Dropitem异常，使用类进行定义，**接口实例如下**；
```
import openpyxl #为了使用exle保存文件，非管道必须
  class myPipeline(object):
    
    def __init__(self):#可定义多个函数
      wb = openpyx.Workbook()
      ws = wb.active
      ws.title = 'Top200'
      ws.append('tiele','评分'，'主题')

    def close_spider(self,spider):
      #关闭时自动调用，钩子方法。
        self.wb.save('电影数据.xlsx')

    def open_spider(self,spider):
      #启动时自动调用
      pass

    def process_item(self,item,spider):
     #process_item处理item
      return item
```
### **管道类型**
 + 过滤性管道：raise DropItem(),丢弃item数据。
 + 加工管道：c = a+b
 + 储存管道：定义三大方法，初始化，打开，关闭。

## middlewares.py中间件(都是钩子)
### 钩子定义
+ 钩子函数(方法)<=>回调函数：不是我们去调用而是主函数需要时自动去调用。
### 下载中间件
```
class SpidernameDownloaderMiddleware:
  def from_carwler(cls,crawler):
```
+ 添加cookies认证(钩子方法)：
```
    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        request.cookies = cookies_dict
        return None
```

## setting.py配置文件

### spider与管道的对应 
+ 通过setting.py文件指定。
  - #ITEM_PIPELINES = {
    "liemr006.pipelines.Liemr006Pipeline": 300,
   }
  - **优先级**：300自定义数字当有多个管道时数字小优先级高。

## scrapy的工程管理与服务器部署
+ 满足了爬虫的持续迭代需求，使用scrapyd管理工具。
+ scrapyd可通过任何可连接到服务器的PC来控制爬虫，可看做scrapy的web封装。**拥有版本控制功能由Restful API提供**
+ 

## 爬网的建立
+ URL的拼接：url = response.urljoin(helf.extract())其中helf为从网页得到的URL挺换的片段。
+ yield Request(url=url)生成新的URL使用Request返回。
## 启动scrapy
+  scrapy crawl project_name
   - -o选项，输出到文件eg： scrapy crawl -o file.json
   -   
# CMD命令
1. 设定打开即以管理员身份运行：搜索框输入cmd->打开文件位置->属性->高级属性(管理员)
2. ipconfig 查看本机IP等
+ 文件管理
 - **切换工作目录**：先切换磁盘目录(输入D:)，输入cd python
 - dir 查看目录文件
 - md+文件夹名，创建文件夹
 - move+文件名+路径名，移动文件。
 - copy+文件名+路径名，复制文件。
 - del+文件名，删除文件。
+ 进程管理
  - tasklist 查询当前进程
  - 强制关闭进程：tasklist /f/pid 进程号码
+ ping 网址
  - 长时间打开网页导致卡顿，可用ipconfig /flushdns
  - **tracert 网址**：查看挑转了几个网关，可定位交换机位置。
## Beautiful Soup
1. 解析HTML文件 soup = BeautifulSoup(filename,"lxml")(可使用lxml(快速，较高容错，需要C语言库),xml(速度快，需要C库，支持XML),html.parser(标准库，均衡),html5lib（容错最高，可生成HTML5，但运行慢扩展差）)
2. 查找标签： soup.head(获取head标签信息) title标签  //.body(主体).b(主体一个部分的标签) .getText(获取标签的值)
3. 精准查找：
  - soup.find-all('a',id="try")属性定位，有属性时可用。
  - soup.find-all('a',class-="efg",id="try")多属性一起定位，更精准。
  - soup.find-all('a',href == re.compile("aaa"))正则匹配
  1. CSS选择
  - 通过id查找：soup.select("#try3"）。
- 通过class 查找：soup.selcct(".efg"）。
-  通过属性查找：soup.select(“a[class="efg"]1）。
## 对文件的操作：

## 数据读写库
### CSV读取与写入
1. import csv
2. csvfile = open('new.csv','w',newline='')#nwelin=''表示每行不用空行隔开，存在new.csv则打开文件否则创建该文件。
3. writer=csv.writer(csvfile)//加载文件到CSV对象中
4. writer.writerow(['afds','afdasf','dsaf])//写入一行数据
5. data = {((‘法’，‘啊’，‘阿帆’)，
            (‘啊’，‘阿帆’，‘发大水发')
    }
    writer.writerows(data)
csvfile.close()
6. reder(*fp)以列表形式输出
   DictReader(*fp)以字典形式输出
### Word
1. pip install python-word
2. 数据写入
from docx import Document
from docx.shared import Inches
2.1. 创建对象 document = Document()
2.2. 添加标题，其中“0”代表标题类型，共有4种类型，具体可在Word的“开始”→“样
式”中查看，document.add_heading('Python 爬虫"，0)
2.3. 添加正文内容并设置部分内容格式
p= document.add paragraph('Python 爬虫开发-1')
2.4. 设置内容加粗 p.runs[0].bold = True
2.5. 添加内容并加粗 p.add_run('数据存储-').bold = True
2.6. 添加内容 p.add run('Word-')
2.7. 添加内容并设置字体斜体 p.add_run('存储实例。').italic = True
2.8. 添加正文，设置“样式”→“明显引用”
document.add_paragraph('样式'-'明显引用'，style='IntenseQuote')
2.9. 添加正文，设置“项目符号” document.add_paragraph(
  'porjet符号1',style='ListNumber'
)
2.10. 添加图片 document.add_picture('test.png',width=Inches(1.25))
2.11. 添加表格 table = document.add table(rows=1, cols=3)
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'Qty'hdr_cells[1].text = 'Id'hdr_cells[21.text ='Desc'for item in range(2):
row_cells = table.add _row().cells
row_cells[0].text = 'a'
row_cells[11.text = 'b"row_cells[2].text ='c"
2.12 保存文件
document.add page_break()
document.save('test.docx')
### Excel
1. pip install xlrd
2.  pip install xlwt
新建一个Excel文件 wb=xlwt.Workbook()
新建一个Sheet ws= wb.add_sheet('Python', cell_overwrite_ok=True)
定义字体对齐方式对象 alignment = xlwt.Alignment()
设置水平方向  HORZ GENERAL, HORZ_LEFT, HORZ CENTER, HORZ_RIGHT, HORZ_FILLED HORZ_JUSTIFIED, HORZ CENTER ACROSS SEL, HORZ DISTRIBUTED
alignment.horz = xlwt.Alignment.HORZ CENTER
设置垂直方向  VERT_TOP, VERT_CENTER, VERT_ BOTTOM, VERT_JUSTIFIED, VERT_DISTRIBUTED
alignment.vert = xlwt.Alignment. VERT_CENTER
定义格式对象
style = xlwt.XFStyle()
style.alignment = alignment
合并单元格write _merge（开始行，结束行，开始列，结束列，内容，格式）
ws.write merge(0， 0， 0，5, 'Python 网络爬虫'，style)
写入数据 wb.write（行，列，内容）
    for i in range(2, 7):
      for k in range(5):
          ws.write(i, k, i+k)# Excel 公式 xlwt.Formula
    ws.write(i, 5, xlwt.Formula('SUM(A'+str(i+1)+':E'+str
插入图片，insert_bitmap(img， x， y， x1， yl， scale_x=0.8, scale_y=1)
图片格式必须为bmp
×表示行数，y表示列数
x1表示相对原来位置向下偏移的像素#y1表示相对原来位置向右偏移的像素#Scale_x、scale_y缩放比例
ws.insert _bitmap('E:\\test.bmp',9, 1, 2, 2, scale_x=0.3, scale_Y=0.3)
## Tkinter图形用户界面
1. import tkinter
win = tkinter.Tk ()#创建新窗口
win.title( 'Hellowe' )#窗口标题
win.geometry('600x300')#原始大小
win.minsize('200','100')#最小大小
win.maxsize('800','800')
win.mainloop()#显示窗口
1. tkinter组件：


# HTML(以html为根元素的文档树)
## 基础知识
html根标签：所有内容，都必须要写在该标签里面。
head是头部标签：用于导入外部的资源信息和描述网页自身信息
title标题标签，它里面的内容会在网页的浏览器的选项卡上显示
body主体标签，网页中能看到到所有内容，都要写在该标签里面
## 标签
+ meta标签提供关于HTML文档的元数据。元数据不会显示在页面上，但是对于机器是可读的。**为单闭和标签**。
  - eg：\<meta charset='utf-8' />编码声明。
  - eg： \<meta name="keywords" content="web前端" />页面关键词。
  - eg：\<meta name="description" content="Web前端" />网页描述
  - eg：\<meta http-equiv="refresh" content="30;url=" />content内的数字代表时间（秒），既多少时间后刷新。如果加url,则会重定向到指定网页
+ <link>标签用于导入外部资源，比如：网页选项卡图标，外部样式
+ h1~h6: 是网页的内容标题标签，h1最大，h6最小。标题标签的特点是：字体会加粗
+ p: 段落标签，用于描述一段内容 
+ hr: 是水平线标签，用于对网页中的内容进行分隔
+ br: 手动换行,在网页中无论打多少空格，或者换多少次行，默认都只是一个空格.
+ strong是加粗标签，em倾斜标签。
+ img是图片标签
  - **src属性设置图片的地址eg：\<img src="./img/bkhm.jpg">**
  - title属性设置鼠标悬停提示信息
  - alt属性设置图片的替代文字，当图片无法显示时，显示对应的文字
  - width属性设置图片宽度，height属性设置图片高度，如果只设置了其中一个属性，另一个属性会等比缩放 
+ a:链接标签，页面间链接，锚链接，功能性链接。
  - 1.页面间链接：href属性设置链接的地址，可以是本地地址，也可以是网络地址。target属性设置目标窗口打开的位置，属性值_self替换自身窗口，_blank是打开新窗口。
  - 2.锚链接：用于当前页面的跳转，从页面的某个区域，跳转到另一个区域通常需要两个a标签，一个a标签通过name属性设置锚标记，另一个a标签通过href属性跳转到对应的锚标记处
  注意：href属性值需要加一个#号
  - 3.功能性链接：利用超链接打开本地的应用
+ div是分区标签，是一个块级标签，通常用于网页的布局
  span是范围标签，是一个行级标签，通常用于突出显示段落中的部分内容
+ 标签分类：
所有的标签可以分为两类：块级标签和行级标签(内联标签)
块级标签：该元素独占一行(h2,p)
注意：通常情况下，行级标签要放在块级标签里面使用
注意：p标签不能嵌套p标签




# CSS(html的样式表)
+ 基本语言格式：容器元素在前，子元素在后。  
## 数据分类
+ 结构化数据：数据库文件
+ 半结构化数据：有基本结构的数据(HTML、JSON)
+ 非结构化数据：无任何结构的数据
## 路径
+ 绝对路径：协议名://主机名：端口/路径
  - 协议名(schema)：http、https、file(本地网页)
  - 主机名(host):域名、IP地址
  - 端口号(port不写为默认值):http默认80，HTTPS默认443
  - 路径(path)
## 标签
+ 关系(无绝对关系只有相对关系)：
  - 嵌套关系
  - 并列关系
## CSS选择器
### 常见元素与属性
+ 元素
  - 元素都有属性(attribute)和文本(text)
  - a元素 -> 网络链接元素
+ 属性
  - string字符属性，查看数据去除了标签。 
### 标签选择器：通过标签来选择
  - 格式：selector('str').get()选择第一个，getall选择所有
  - **与浏览器配合**：元素面板Ctrl+F打开搜索框，输入标签可查看所有符合的标签.(**注：浏览器的数据经过渲染可能与代码选择的结果不同**)
### 类选择器
  - 格式：css('.tep').getall()//提取所有有top属性的标签
### ID选择器(ID通常具有唯一性)
  - 格式: css('#ID').getall() 

# Pythoncharm技巧
1.	安装汉化插件可以使其汉化
2.	要让所有的项目保存在一个文件夹A中，先在A中建立一新的文件夹B将新项目的地址设为B之后每次添加新的都是在A中，再将B删除即可。
# 数据的可视化


# 其他知识
1. API：程序编程接口。(别人写好的程序提供给你一个使用接口)
