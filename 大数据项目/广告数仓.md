[toc]

***

# 广告数仓
+ DataX工具是否需要在所有机器上安装？
  - 不需要，只需要在主节点上安装
  
+ 集群非正常关闭后，kafka无法启动（或多台机器仅一台成功）
  - 关闭集群，删除zeekoop中保存的kafka集群元数据，重新启动zookeeper
  - 路径：/opt/module/zookeeper/Data/version-2

## clickhouse
+ clickhouse服务
  - 启动：sudo systemctl start clickhouse-server
  - 停止：sudo systemctl stop clickhouse-server
  - 重启：sudo systemctl restart clickhouse-server
  - 查看状态：sudo systemctl status clickhouse-server
+ clickhouse客户端
  - 启动：clickhouse-client -m

+ 测试jdbc
clickhouse-client \
  --host hadoop102 \
  --port 8123 \
  --database ad_report \
  --user default \       
  --query "SHOW TABLES"


## Dolphinscheduler
+ 网站：hadoop102:12345/dolphinscheduler/ui/
+  依赖关系：依赖zookeeper
+ DolphinScheduler 的启停
  /opt/module/dolphinscheduler/bin/start-all.sh
  
+ 任务运行无实例：资源是否足够
+ 同步到CK的spark任务参数
  ```
  --hive_db ad   \
  --hive_table dwd_ad_event_inc \
  --hive_partition 2023-01-07   \
  --ck_url  jdbc:clickhouse://hadoop102:8123/ad_report   \
  --ck_table dwd_ad_event_inc   \
  --batch_size 1000
  ```
+ DW环境
  ```
  export HADOOP_HOME=/opt/module/hadoop
  export HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop
  export SPARK_HOME=/opt/module/spark
  export SPARK_HOME1=/opt/module/spark
  export SPARK_HOME2=/opt/module/spark
  export JAVA_HOME=/opt/module/jdk1.8.0_212
  export HIVE_HOME=/opt/module/hive
  export DATAX_HOME=/opt/module/datax

  export PATH=$PATH:$HADOOP_HOME/bin:$SPARK_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$DATAX_HOME/bin
  ```
### 增减master、worker节点
  - 增加节点
    - 1. 修改配置文件
      - 修改/opt/module/dolphinscheduler/conf/config/install_config.conf 
    - 2. 重启dolphinscheduler

## FineBI
+ 访问网站：http://hadoop102:37799/webroot/decision
+ 依赖：hadoop集群 + clickhouse + hive 
+ FineBI 的启停：cd /opt/module/FineBI6.0/bin
  -  nohup ./finebi &
  - kill -9 
  - 进程名：UnixLauncher

## 采集新的一天数据
### 数据生成
+ 启动日志采集通道，包括Kafka、Flume：hadoop102：
  myhadoop start
  zk.sh start
  kfk.sh start

+ 修改日志模拟器配置文件：(hadoop102和hadoop103)
  - 或者运行脚本：generate_one_day_logs.sh 2025-06-04
  - vim/opt/module/ad_mock/nginxLogGen.setting
    #生成数据的开始时间
    startTime = 2023-01-08 00:00:00
    #生成数据的结束时间
    endTime = 2023-01-09 00:00:00
    scp  nginxLogGen.setting  hadoop103:/opt/module/ad_mock

+ 启动采集：ad_f1.sh start    ad_f2 start

+ 执行日志生成脚本
  ad_mock.sh
  观察HDFS/origin_data/ad/log/ad_log/XXX文件是否生成


+ 等待一段时间：hadoop104收集写入日志需要时间，可通过观察hadoop104的CPU使用大概判断是否运行完。
  - 关闭flume采集：ad_f1.sh stop   ad_f2.sh stop
  - 再次启动采集（保证完全采集数据）：ad_f1.sh start    ad_f2 start
+ 关闭flume、kafka：ad_f1.sh stop   ad_f2.sh stop    kfk stop


### 数据解析、写入数据库(hadoop102)
+ 在前面的基础上开启hive、clickhouse
  - hiveservices.sh start
  - sudo systemctl start clickhouse-server

+ 方式一
  - cd /opt/module/spark/bin
  - 执行脚本：load_one_day_data.sh 2025-06-03

+ 方式二
  + 将数据解析加载到Hive中，生成用于分析的dwd层表
    - mysql_to_hdfs_full.sh all 2025-05-26
      - origin_data/ad/db/XXX_full
    - ad_hdfs_to_ods.sh all 2025-05-26
      - hive/ad/ods_XXX_full
    - ad_ods_to_dim.sh all 2025-05-26
      - hive/ad/dim_XXX_full
    - ad_ods_to_dwd.sh all 2025-06-02
      - hive/ad/dwd_ad_event_inc

  + 将hive中的数据导入到ClickHouse中，以后利用ClickHouse作为分析引擎：CK——services、hive、spark
    - cd /opt/module/spark/bin
    ```
      spark-submit   \
      --class com.atguigu.ad.spark.HiveToClickhouse \
      --master yarn   \
      ad_hive_to_clickhouse-1.0-SNAPSHOT-jar-with-dependencies.jar   \
      --hive_db ad   \
      --hive_table dwd_ad_event_inc \
      --hive_partition 2025-06-02   \
      --ck_url  jdbc:clickhouse://hadoop102:8123/ad_report   \
      --ck_table dwd_ad_event_inc   \
      --batch_size 1000
    ```
+ 使用finebi查看数据
### 问题解决
+ 执行该任务时访问不到yarn，一直显示连接0.0.0.0.0:8032超时
  - 修改配置文件：/opt/module/spark/conf/spark-defaults.conf，因其使用的hadoop配置路径为绝对路径，而安装时hadoop文件夹保留了版本号，导致spark找不到hadoop配置文件。
  - 报错：
  ```
  spark-submit   \
  --class com.atguigu.ad.spark.HiveToClickhouse \
  --master yarn   \
  ad_hive_to_clickhouse-1.0-SNAPSHOT-jar-with-dependencies.jar   \
  --hive_db ad   \
  --hive_table dwd_ad_event_inc \
  --hive_partition 2023-05-28   \
  --ck_url  jdbc:clickhouse://hadoop102:8123/ad_report   \
  --ck_table dwd_ad_event_inc   \
  --batch_size 1000

  25/05/30 12:21:14 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:15 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:16 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  zk25/05/30 12:21:17 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:18 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:19 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:20 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:21 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:22 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:23 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
  25/05/30 12:21:23 INFO RetryInvocationHandler: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over null after 25 failover attempts. Trying to failover after sleeping for 39742ms.
  ```
+ 时间戳：自1970-01-01 00:00:00 0毫秒0时区开始计算
  - 是一个绝对的概念，即在任何地方加上考虑时区的情况下以当地时间转换为时间戳后，所得到的时间戳值都是一样的。


## Spark安装
+ 必须配置env_sh:


# 1. 获取完整日志
yarn logs -applicationId <your_application_id>

# 2. 仅查看错误日志（推荐）
yarn logs -applicationId application_1684758308661_0001 | grep -i -A 50 -B 10 'error\|exception'

# 示例：
yarn logs -applicationId application_1684758308661_0001 | grep -i -A 50 -B 10 'error\|exception'









## 网络问题
+ 查看服务状态：sudo systemctl status network.service

+ 网络配置中存在重复的IP地址或路由
  - 解决：
    - 1. 清除所有IP地址配置
    sudo ip addr flush dev eth0  # 替换为您的物理网卡名（如ens33）
    - 2. 删除所有路由
    sudo ip route flush all
    - 3. 停止冲突服务
    sudo systemctl stop NetworkManager
    - 4. 重启网络服务
    sudo systemctl restart network

    sudo ip addr flush dev ens33
    sudo ip route flush all
    sudo systemctl stop NetworkManager
    sudo systemctl restart network
  ```
  [atguigu@hadoop102 ~]$ systemctl status network.service
  ● network.service - LSB: Bring up/down networking
    Loaded: loaded (/etc/rc.d/init.d/network; bad; vendor preset: disabled)
    Active: failed (Result: exit-code) since 三 2025-05-28 19:54:45 CST; 1min 28s ago
      Docs: man:systemd-sysv-generator(8)
    Process: 3728 ExecStart=/etc/rc.d/init.d/network start (code=exited, status=1/FAILURE)

  5月 28 19:54:45 hadoop102 network[3728]: RTNETLINK answers: File exists
  5月 28 19:54:45 hadoop102 network[3728]: RTNETLINK answers: File exists
  5月 28 19:54:45 hadoop102 network[3728]: RTNETLINK answers: File exists
  5月 28 19:54:45 hadoop102 network[3728]: RTNETLINK answers: File exists
  5月 28 19:54:45 hadoop102 network[3728]: RTNETLINK answers: File exists
  5月 28 19:54:45 hadoop102 network[3728]: RTNETLINK answers: File exists
  5月 28 19:54:45 hadoop102 systemd[1]: network.service: control process exi...=1
  5月 28 19:54:45 hadoop102 systemd[1]: Failed to start LSB: Bring up/down n...g.
  5月 28 19:54:45 hadoop102 systemd[1]: Unit network.service entered failed ...e.
  5月 28 19:54:45 hadoop102 systemd[1]: network.service failed.
  Hint: Some lines were ellipsized, use -l to show in full.
  ```