[toc]

---
# 面试
+ 自我介绍：面试官你好，我是XXX，来自贵州民族大学数据科学与大数据专业的应届毕业生，在校期间参加了两个项目一个是广告效果投放分析数仓，一个是电商实时数仓。
## 项目
+ hadoop集群：硬盘的读写速度和网络带宽影响集群吞吐量的两个核心因素。
+ 报表：仪表盘是报表的一种表现形式。
+ 建模工具是什么？
  - PowerDesigner/SQLYog/EZDML

### 广告效果投放分析数仓
+ 挑战：
  - 解析IP：一开始采用的是高德的IP解析，因为需要一个个发请求但是解析速度慢，后采用github上的一个ip数据库查询解析
  - 异常流量标记：一般的爬虫用请求头、高频访问标记即可，到时高度伪装的不好比较，采用同样设备id高频、周期访问标记。
### 电商实时数仓
+ ETL
  - 解析为JSON对象，若无法解析则将其发送到侧输出流。
  - 主流分流，利用侧输出流将JOSN中不同字段写入kfk中的不同主题（启动、曝光、动作、错误，分别对应一张事实表）
+ DWS：消费kfk中dwd层数据，开窗聚合后写入dws层
+ 业务数据的有序性： maxwell配置，指定生产者分区的key为 table
+ 维度表为什么保存在HBase中：事实表存Kafka、维度表存Hbase，基于热存储加载维表的join方案：随机查、长远考虑、适合实时读写
+ mysql存一张表来动态配置：广播状态：FlinkCDC使用sql同步此配置表
+ 挑战：
  - 每次查询需要连接HBase，数据传输需要序列化、反序列化网络传输等导致性能瓶颈，通过Redis缓存查询结果解决（设置数据保存一天）。
  - 外网访问：采用钉钉的内网穿透工具完成的。
+ dws选择了clickhouse：适合大宽表、数据量多、聚合统计分析，且已经不需要join。
  - ReplacingMergeTree只能保证最终一致性，查询时的sql语法加上去重逻辑
+ 监控：Flink和ClickHouse都使用了Prometheus + Grafana

## 数据分层理论
+ ODS：数据源层，数据仓库源头系统，直接从数据源抽取数据。
+ DWD：数据仓库明细层，对ODS层数据进行清洗，去除空值、脏数据、重复数据等。
+ DWS：数据仓库汇总层，对DWD层数据进行汇总，将多个事实表进行聚合，形成一张事实表。
+ ADS：数据仓库应用层，根据业务需求，对DWS层数据进行统计、分析，形成报表。
## Linux
+ 常用高级命令
  - ps:查看进程，-e查看所有进程，-f查看进程详细信息，-aux查看所有进程
  - iotop:查看磁盘IO，-o查看io占用较高的进程
  - uptime:查看系统负载
  - top:查看内存使用情况
  - df -h:查看磁盘空间
  - netstat -tunlp | grep 端口号
+ shell常用工具：
  - awk：文本分析工具
  - sed：流编辑器
  - sort：排序
  - cut：剪切
+ shell脚本：
  - 单引号不取变量值，双引号取变量值，反引号执行引号中命令
  - 双引号内部嵌套单引号，取出变量值
  - 单引号内部嵌套双引号，不取出变量值
+ tail -f -n 100 server.log：实时查看最后100行日志

## Hadoop3.x
+ 端口：
  - 9870：namenode外部webui端口（HDFS）
  - 8088：yarn的webui端口（MR执行端口）
  - 19888：历史任务服务器端口
  - 8020：namenode内部通信端口（客户端访问集群）
+ 配置文件：core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、workers
+ 慢磁盘找出：超过心跳时间未联系。
+ 集群搭建过程：
  - JDK安装、配置ssh、配置核心配置文件、格式化NN
+ hadoop宕机
  - MR导致：控制Yarn同时运行的任务数，和每个任务申请的最大内存
  - 写入文件过快造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。例如，可以调整Flume每批次拉取数据量的大小参数batchsize
## HDFS
+ hdfs读写流程
  - 读取：客户端向namenode发送请求，namenode返回数据块的位置信息，客户端根据位置信息向datanode读取数据。
  - 写入：客户端向namenode发送写请求，NN响应返回确认响应，client请求分配Datanode，NN返回一组datanode地址，client向DN1请求建立传输通道（DN1向DN2传递此请求），DN3到DN1依次返回应答成功，client向DN1发送数据，DN1向DN2、DN3传递数据，client向NN发送数据传输完成。
+ 储存
  - 一个文件块128M（临近硬盘读取速度，企业可能256/512），**对应一个NN一个150字节。**
  - 计算：一块 -> 一个maptask -> 1G内存 -> 100万个block
+ 小文件归档
  - 将多个小文件打包成一个文件，减少namenode的内存占用。
  - 切片：多个小文件统一切片。（默认一个文件一个切片）
  - 大量小文件job，可开启JVM重用。（任务完成才释放task卡槽）
+ 纠删码：将数据切分为3个数据单元2个校验单元，丢失两个可计算找回。
+ 异构存储（冷热数据分离）：期望经常使用的数据存储在固态硬盘或者内存镜像硬盘；不经常使用的历史数据存储在老旧的破旧硬盘。
## MapReduce
+ shuffle概念：MP输出到reduce的中间混洗程。
+ shuffle调优：
  - map端、reduce端增大内存。（1cup -> 4G内存,128M数据 -> 1G内存）
  - 避免数据倾斜，自定义分区
  - 采用数据压缩的方式，减少磁盘 IO 的的时间
  - 增加CPU核数，提高计算能力解决计算密集型任务。
  - 不影响业务的前提下，采用conbiner预聚合（job.setConbinerClass）
  - 合理设置 Map 和 Reduce 数：两个都不能设置太少，也不能设置太多。太少，会导致 Task 等待，延长处理时间；太多，会导致 Map、Reduce 任务间竞争资源，造成处理超时等错误。
+ 数据倾斜，某个task执行时间很长
## YARN
+ 工作机制：客户端向resourceManager申请一个Application，RM生成一个Task用于调度，并返回一个ApplicationID、资源提交路径，客户端提交job需要的资源（job.submit()后生成Job.split/Job.xml/Wc.jar）到RM,RM将资源分配给Task，Task启动1个容器并下载job资源到本地，第一个容器再向RM申请多个容器运行具体的MP、RD任务。最后运行完毕，MR向RM释放资源。
+ yarn调度器：
  - FIFO：先进先出，适合短作业。
  - 容量（Capacity）：支持多队列，资源不足可借用其他队列，先进先出。
  - 公平（Fair）：支持多队列，资源不足可借用其他队列，每个任务占用资源比例一样。
## Flink
+ Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。
+ 基本架构：Client（解析main方法生成JobGraph）->JobManager(生成执行图)->TaskManager(实际负责执行计算的Worker)
+ task、subtask：算子并行实例，一个task包含多个subtask。
+ 算子：map用return返回一进一出，faltmap用collector调用几次输出几次。
  - 重分区算子：keyby、shuffle、reblance
+ 并行度：算子>全局>命令行提交参数>配置文件
+ 部署模式：
  - 本地模式：
  - standalone模式：
  - Yarn模式：
    - 会话模式：
    - per_job模式：: 独享一个flink集群资源，动词申请task
    - application模式：独享一个flink集群资源，但是代码在jvm中运行
  - K8s模式： 
+  Flink的精准一次Exactly-once：Flink 通过实现两阶段提交和状态保存来实现端到端的一致性语义。
  - 开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面预提交（preCommit）
  - 将内存中缓存的数据写入文件并关闭正式提交（commit）将之前写完的临时文件放入目标目录下。（这代表着最终的数据会有一些延迟丢弃（abort）丢弃临时文件若失败发生在预提交成功后）
  - 正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。
+ 分区策略：
  - 随机分配（Random）：分配后均匀分布的。
  - 轮询分配（Round-Robin）：按照先后顺序将数据做依次分发
  - 重缩放（Rescale）：根据下游的并行度，将数据做分发，下游并行度是下游并行度的整数倍。
+ watermark（水位线）： Flink 用来处理乱序事件的机制
  - 用一个时间戳插入数据流中，可以解决乱序问题。
  - 可用来触发窗口、定时器，事件时间小于waterMark称为达到数据。
  - 传递：一对多：广播、多对一：最小有效、多对多：拆分
  - 生成原理：当前最大事件时间 - 允许延迟时间 - 1ms
  - 生成方式：200ms周期生成、间歇生成（水位线不是flink独有，来源谷歌论文）
+ Flink 中的 Window 出现了数据倾斜
  - 预聚合
  - 重新选择聚合key（eg：北京的拆分维北京上午、下午）
+ 合流：
  - connect：合并两个流，两个流类型可以不同，合并后类型为Tuple2
  - union：合并两个流，两个流类型必须相同，合并后类型为DataStream
  - join：窗口用
  - interval join：底层用keyby + connnect，拿不到join不上的数据
+ 窗口：时间（滚动、滑动、会话）窗口，数据条数（滚动、滑动）
  - 组成：分配器、触发器、驱逐器、窗口函数（增量聚合、全窗口）
  - 划分：start、end（start=数据时间戳截取窗口长度整数倍），end = start + 窗口函数
  - 触发：时间 >= MaxTs
+ 状态：托管状态算子（没有keyby状态）、键控状态、自定义状态。用hashmap保存
  - 状态后端：本地状态保存在内存、checkpoint保存文件
+ Exactly-Once精准一次性：
  - source端：数据可重发，kfk满足
  - flink端：checkpoit（分布式快照）
  - sink端：幂等、事务（kfk：2pc两阶段提交、hb幂等、Redis幂等）
+ Flink分布式快照（容错机制，实现精准一次的核心）
  - JobManager向所有数据源发出插入barriers指令，source记录自身状态并注入barriers。
  - Barrier 到达算子时，触发状态保存并向下游转发。
    - 对齐：当算子收到 来自所有输入通道（Input Channel）的 Barrier 时，才触发本地快照。在等待对齐期间：缓存 来自 尚未对齐通道 的数据（避免干扰快照边界）。继续处理 来自 已对齐通道 的数据（保证吞吐量）
  - Sink 算子 收到 Barrier 并完成本地状态保存后，向 JobManager发送确认信息。
  - JobManager 收到所有算子的确认信息后，标记快照完成。
+  Flink 是如何做容错的？
 - Flink 实现容错主要靠强大的 CheckPoint 机制和 State 机制。Checkpoint 负责定时制作分布式快照、对程序中的状态进行备份；State 用来存储计算过程中的中间状态
+ checkpoint参数设置：1~10分钟，超时0.5~2倍之间，最小ck等待间隔0.5~1倍之间，重启策略，保存
+ flink时间语义：
  - 事件时间：数据产生的时间
  - 处理时间：数据被处理的时间
  - 提取时间：数据进入flink的时间
+ Flink的CEP：支持水位线
  - 复杂事件处理，用于对数据流进行模式匹配，可以用于实时监控告警、日志解析等场景。
  - 一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据 —— 满足规则的复杂事件
+ Flink 的序列化
  - Flink 内部采用自己实现的 TypeInformation 来描述数据类型，TypeInformation 支持以下几种类型：java、string的的基本类型及其数组、Hadoop Writable 接口的实现类、任意的 Flink Tuple 类型(支持 Tuple1 to Tuple25)、任意的 Scala CaseClass(包括 Scala tuples)、任意的 POJO (Java or Scala)
+ Flink 是如何处理反压的？
  - flink内部是基于producer-consumer 模型来进行消息传递，使用了高效有界的分布式阻塞队列，下游消费者消费变慢，上游就会受到阻塞，逐级反压。
+ Operator Chains（算子链）
  - Flink 会尽可能地将 operator 的 subtask 链接（chain）在一起形成 task。每个 task 在一个线程中执行。将 operators 链接成 task 是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。
  - 连接条件：one to one、并行度一致
+ flinkSQL工作机制：
  - SQL解析为语法树AST -> 生成逻辑计划 -> 优化逻辑计划 -> 生成物理计划 -> 生成可执行的job
+ FQL的优化器：
  - RBO（基于规则）：满足设定规则就转换（如：聚合下移、谓词下推、子查询内联转join）
  - CBO（基于成本）：找到执行计划最小的计划
+ 维度表join方案
  - 预加载： open（）方法，查询维表，存储下来 ==》 定时查询
  - 存在外部系统redis、hbase等缓存 异步查询： 异步io功能
  - 广播维表
  - Temporal join：外部存储，connector创建

## Spark
+ shuffle优化
  - 
## 数据同步工具
+ maxwell
  - 支持断点续传，保证至少一次数据不丢失
  - 通过监听mysql binlog日志，将数据变化同步。（底层是mysql主从复制模拟从表）
  - 资源占用小，数据转发后无需先进行格式转换。
+ FlinkCDC
  - 也需要开启MySQL Binlog
  - 能将CDC功能与flink的流处理能力结合，直接在flink作业中进行数据处理
## Hive
+ 用户自定义函数
  - UDF：自定义函数
  - UDAF：多进一出
  - UDTF：一进多出，出的是表。
+ 建表：create、select、like（仅有结构没有索引）
+ NULL值处理：以\N保存。
+ hive内存溢出：
  - 数据倾斜
  - 内存不足
+ 4个BY
  - order by：全局排序，只有一个reduce，效率低。
  - sort by：局部排序，每个reduce内部排序。
  - distribute by：分区排序，按指定字段分区。
  - cluster by：分区排序，按指定字段分区，分区内部排序。**仅可倒序排序**
+ hive性能调优：分区表、分桶表、HQL优化、JOIN优化、并行度设置、优化设置MP、RD个数。
+ join：小表在左大表在右，左表会被加载到内存中。
+ 简单hql无需mp，用Fetch Task拉取即可。
+ 内部表语外部表：外部表数据不由hive管，仅管理元数据。
+ HQL转换为mp过程：生成语法树 -> 生成操作树 -> 逻辑优化 -> 生成mp -> 物理优化 -> 生成执行计划 
+ 窗口函数：over
+ 数据保存在HDFS上，hive不建议修改其中的数据，hive但数据规模大时延时反而较低。
+ hive分隔符采用\001,可以设置参数使得分隔符不导入。
## Kafka
+ 组成：生产者、消费者、broker、zookeeper
+ kfk选举：broker启动后会在zk中注册，谁先注册谁是leader，leader用ISR队列（当机器延迟过高则加入OSR）保存注册顺序，leader挂了轮询数组找到新的leader。通过epoch任期解决脑裂。
+ kfk测压：官方自带压力测试脚本kafka-consumer-perf-test.sh、kafka-producer-perf-test.sh，CPU，内存，网络 IO一般是网络。
+ kfk配置：
  - 机器数 =  2 *（峰值生产速度 * 副本数 / 100）+ 1
  - 日志默认保存7天
  - 磁盘大小：天的数据量100g * 2个副本 * 保存天数 / 70%
  - 分区数： = 机器数，3~7个
  - 副本两个
  - 数据条数峰值 = 均值 * （2~20）
  - 应答策略Ack=： 0：不等待。 1：等待leader应答。 -1：等待leader和follower都应答
  - 主题数 = 日志类型数
+ 分区器的分配策略（生产者端）：
  - 指定分区：指定分区号
  - 按key分区：根据key的hash值取模
  - 粘性分区：随机选择一个分区，持续写入到批次满或超时（无key数据）
  - 自定义
+ 消费者分区策略：
  - 轮询RangeAssignor：分区数 / 消费者数 = 分配到消费者个数,余数分配到前面消费者
  - 多轮轮询RoundRobinAssignor：先分组在轮询
  - 粘性分区：先进行多轮轮询，当消费者变动时触发再平衡，以最小的调整代价调整分区。
  - 自定义：eg每个小时一个主题
+ kfk精准一次消费：
  - 幂等生产者：生成者id唯一、消息序列号唯一。
  - 事务提交：生产者提交多个分区消息，采用事务提交。
  - 事务型消费者，仅读取已经提交的数据
+ 数据可靠性：副本数 >= 2
+ 提高kfk吞吐量：增大缓存、扩大批次大学、采用压缩储存、提高延时
+ 数据有序：单分区有序，若需要全局有序可设置分区数=1
+ 重复数据处理：幂等、事务
+ 数据存储：.log、.index(稀疏索引，4KB一条)、.timeindex(时间索引)、.snapshot、.offsets
+ kfk高效读写：集群且支持分区、采用稀疏索引、零拷贝(数据传输不走broker，通过OS特权指令直接拷贝)、页缓存(数据先写入页缓存，再由OS写入磁盘。)、顺写日志
+ 数据积压问题解决：增加分区数、消费者数、处理批次大小
+ kfk参数优化：
  - 提高延时、设置副本数Replica、
  - 生产端数据压缩：commpression默认是none，可设置lz4、gzip、snappy
  - 默认内存1个G，生产环境尽量不要超过6个G
  - 单条日志最大值是1M，replica.fetch.max.bytes可设置大点，但是注意可能需要同时调整kfk集群接收数据大小：message.max.bytes。max < fetch
+ kfk的高效读写：
  - 零拷贝：数据传输不走broker，通过OS特权指令直接拷贝。
  - 顺写日志：数据仅追加到文件末尾做顺序写入。
+ kafka过期数据清理：delete：删除，compact：压缩
## flume
+ Flume包括Source/Channel/Sink选型
  - 一个agent包括：source、channel、sink
+ 事务：put事务：从source->channel，Take事务：从channel->sink
+ source：
  - 支持断点续传、多目录
  - 自身不处理数据重复问题，下流处理，若使用事务效率较低。
  - 不支持递归遍历文件夹读取文件，可自定义实现递归 + 读取文件。
+ channel：
  - file 默认100个event，可指定多路径，若在不同盘可提高吞吐量。
  - memory 默认100个event，高速低可靠
  - kafka 数据保存在磁盘，传输速度快 Kafka Channel 大于Memory Channel + Kafka Sink  原因省去了Sink阶段
+ 自定义拦截器
  - ETL拦截器：判断数据是否完整，尽量不做复杂解析防止影响传输速度
  - 时间戳拦截器：将数据时间戳替换为当前时间戳，可同时解决零点漂移问题
  - 重写intercept方法
  - 重写initialize方法
  - public Event intercept(Event event) 处理单个 Event、
  - public List<Event> intercept(List<Event> events) 处理多个 Event，在这个方法中调用 Event intercept(Event event)、 close 方法
  
+ 零点漂移问题解决
  - 描述：数据时间戳与当前时间相差过大（Agent非正常关闭，内存中的event未持久化而丢失）
  - 使用kfk缓冲
+ 采用Ganglia监控器，监控到Flume尝试提交的次数远远大于最终成功的次数，说明Flume运行比较差。主要是内存不够导致的
## zookeeper
+ CAP理论：分布式系统不可能同时满足三点，zk满足CP（选举时不可用）
  - Consistency（一致性）：所有节点保持数据一致的特性。
  - Availability（可用性）：系统提供的服务一直处于可以状态。
  - Partition tolerance（容错性）：分区故障任然可用
+ 选举机制：过半机制，超过半数投票通过。（故集群偶数台）
  - 选举规则：epoch（任期id，投一轮id+1）大的直接胜利否则投票，投票投给myid最大的节点，每台服务器连接不到leader
## Redis
+ 缓存穿透：查询不存在的数据，导致每次都要访问数据库和Redis
  - 布隆过滤器（将可能存在的数据哈希值保存在位图中）、缓存空值（3分钟过期）
+ 缓存击穿，是指一个 key 非常热点，在不停的扛着大并发，当这个 key 在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。
  - 设置热点key永不过期
+ 缓存雪崩：缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上
  - 尽量让失效的时间点不分布在同一个时间点
+ 哨兵模式：复制中反客为主的自动版，如果主机 Down 掉，哨兵会从从机中选择一台作为主机，并将它设置为其他从机的主机，而且如果原来的主机再次启动的话也会成为从机
+ redis 是单线程的，为什么那么快？
  - 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。
  - 数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的。
  - 单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，
+ 查询数据一致性：数据库中数据更新时删除Redis中的数据及Redis中数据24小时过期。
## Doris
+ 功能强大易用（兼容mysql、易运维、拓展，高可用，join强，不依赖其他组件）
+ 集群规模：FE(元data)、BE(data)，FE、BE不可混部，也不要与hadoop集群混布。因为他们都要集群资源。
+ 数据模型：采用聚合中的幂等模型，存放dwd层数据
  - 聚合：聚合模型，将数据按时间分区，分区内数据按key聚合。
  - 明细：明细模型，将数据按时间分区，分区内数据按key存储。
  - 唯一：将数据按时间分区，分区内数据按key存储
+ 优化：调大BE、超时时间
## HBase
+ 结构：HRegionServer、ZK、HDFS。（HR：预写日志、region）
+ 写流程（写比读快）：client -> ZK -> 元数据位置 -> client -> 数据写入预写日志 -> 数据写入写缓冲。
+ 刷写
  - 写满128M，写缓冲区刷写。
  - Reggerve内存满了。
  - 定时刷写、手动刷写
+ 合并：七天一合并。
+ 预分区：建表时必须指定分区（region）
+ 二级索引：rowkey是一级索引，自己维护的是二级索引。
+ rowkey设计原则：
  - 长度适中，不要过长，不要过短。
  - 唯一性，不要重复。
  - 散列性，不要连续。
  - rowkey具体设计
    - 字符串翻转
    - 生成随机数、hash、散列值
## DS
+ 指标挂了，集成报警，报警微信电话等通知。


## 项目经验
+ 集群规划
  - 数据规模：ODS、DWD层数据压缩1/10（LZO压缩），DWS层不压缩但有聚合1/2，ADS忽略不计数仓数据2个副本30%冗余。kfk中保存3天数据2副本30%冗余，flume忽略布局。
  - 集群规模：5~20台之间
  - 集群部署原则：消耗内存的分开、数据管道防止一起、客户机放在1、2台机器方便访问、有依赖关系在同一台 
+ 数仓建模工具：PowerDesigner
+ 分层
  - ODS：原始数据层，数据源直接导入，不做任何处理，或仅做分区表。
  - dim：维度表，保存维度数据。
  - dwd：明细数据层，数据清洗、去重、格式化、字段统一、分区表，保存事实表。
  - dws：数据汇总层，dwd数据维度退化、聚合。
  - ads：数据应用层，dws数据聚合、维度退化、保存维度表。
+ ETL
  - 空值去除，无意义字段过滤
  - 数据清洗手段：sql、HQL、MR，万条数据清洗掉一条合理。
  - 脱敏：对手机号、身份证号等敏感数据脱敏
  - 维度退化：将维度表退化到事实表中，减少join次数。
+ mysql常用：select、group by + having（过滤）、like、distinct、where、left、in、where exists、or、all、any、between...and、over、union、union all（联合查询，两个SQL结果合并为一行）、.{A,B,C}正则、order by(末尾加上DESC降序)、is null、interval 1 day、partition by、
  - **SQL实际执行顺序： from(含join) -> where -> group by -> having -> select ->   distinct -> order by ->  limit、offset**
  - count(distinct id)//统计去重id个数，round(x,n)四舍五入保留n位小数
  - IN、NOT IN（**本质是=、！=运算，在SQL中任何值与null的not in 运算结果都是空值，故使用not in需要保证集合中没有空值**）
+ SQL查询优化
  - 避免使用select *(选择某个表的所有字段可用st.*)，全局扫描太耗费时间，选择运算尽可能先做。
  - 小表驱动大表：left join时小表(数据少、索引完全)在前，大表在后，前为驱动表
  - 用连接查询代替子查询(部分表适用)：多一个子查询多一次连接数据库查询，且子查询需要临时表效率低。
  - **索引调优**：
    - 优化where：避免在where上使用函数或计算(=、<>)，这会导致索引失效，可用>25 AND <= 25代替
    - group by 字段(索引字段)，查询过慢可尝试为该字段加索引,同理保证where条件、join条件、order by字段有索引可用。
    - 使用后导模糊查询：LIKE '%abc' 无法使用索引，但 LIKE 'abc%' 可以。
  - 批量操作：数据连接消耗资源，考虑内存下一次插入500条数据
    - 批量插入，insert into orders values(1,2,3),(4,5,6)
  - 使用limit，减少数据量。（但limit 10000,10不可取,它会拿到10000条数据在返回最后10条数据，用where id代替）
  - 使用exists代替in，in会全表扫描，exists只扫描外表。
  - 确定结果无重复或重复无影响，用union all 代替 union
  - 减少多表查询、子查询(找出公共子查询用CET)、物化视图。
    - 选择和连接合并，先连接后过滤
    - 减少全连接操作，连接条件可以是 a.date - b.date = interval 1 day
  - join的表不宜过多，因为join需要消耗CPU资源。
+ SQL优化原则：**根据日志找到慢sql，sql前加上explain查看执行计划**
  - 如果type是ALL：考虑增加索引。 
  - 如果Extra有Using filesort或Using temporary：优化GROUP BY/ORDER BY，或为排序字段加索引。
  - 如果rows很大：优化WHERE条件，增加更有效的索引。
  - 检查SQL写法，消除SELECT *、不必要的子查询等。

+ 易错点
  - 使用**保留字**作为列名时需要用反引号括起来，字符串、日期直接写需要用单引号括起来。
  - **有聚合函数必须有group**
  - 删除和查询同时进行（可将查询后的表，外面再嵌套一层select即可解决）
  - not in 不可用于含null值的集合，因为null值与任何值比较结果都是null
  - 注意比较两个值大小，当其来源有多个时需要考虑是否使用max、min
  - 子查询中多了;号
    ```
    select st.*
    from student st
    where
    (
      select score  from 
      (select sId,score,cname from SC left join Course c on SC.cId = c.cId) res 
      where cname = "语文" and res.sId = st.sId;
    ) 
    >
    (
      select score  from 
      (select sId,score,cname from SC left join Course c on SC.cId = c.cId) res 
      where cname = "数学" and res.sId = st.sId;
    )
    ```
  - SELECT 子句	必须是标量子查询	为每一行添加动态计算列
  - FROM 子句	表子查询	创建临时表（派生表）供主查询使用
  - WHERE / HAVING 子句	标量、列、行子查询	动态地过滤行或分
  - DML 语句 (INSERT/UPDATE/DELETE)	表子查询或标量子查询	基于复杂条件操作数据
+ 函数与技巧
  - date_format(date_column, '%Y-%m') //**格式化日期，也可用来截取日期**
    - %Y/y：四位数/两位数年份，%M/m：月份，%D/d：日，%H/h：小时，%i：分钟，%s：秒
  -  COUNT(CASE WHEN 年龄 = 15 THEN 1 END) AS 15岁人数,**有条件统计**
  -  使用limit 6,18446744073709551615可跳过前6行取数据
  -  group_concat(distinct product order by product separator ',') as new_column //**将字段拼接成字符串**
  - 正则表达式：where regexp_like(mail, '^[a-zA-Z][a-zA-Z0-9._-]*@leetcode\\.com$', 'c')，或者where mail regexp “正则表达式“
  - select id,"aaa" zm from tab1;//查询所有id，将"aaa"赋值给zm列，常与union联合用于行转列
  - substing_index(str,'/',1)，提取以/为分隔符的第一个字符子串，substing_index("http:/url/user_name", '/', -1)结果为user_name
  - group_concat(distinct product order by product separator ',') as new_col,**指定分隔符将str1和str2拼接为一个字符串**。
    - separator ',' //指定分隔符
    - 相较于concat可排序、指定分隔符。
  - 复合条件：(A=1 and str = 'true') or (A=2 and str = 'false')
  - 匹配多个值：in (1,2,3) ，模糊查询 like '%1%' 
  - join条件：= 、> 、datediff() >10
### 电商数仓
+ DWS
  - 具体宽表名称：用户行为宽表（最宽），商品宽表，访客宽表、活动宽表、优惠卷、地区表等
+ ADS
  - 分析指标：日活、月活、周活、留存、留存率、新增（日、周、年）、转化率（商品详情  =》（0.05-0,1）  加购物车  =》（0.6-0,7）下单   =》（0,9-0,95）  支付）、流失、回流、七天内连续3天登录（点赞、收藏、评价、购买、加购、下单、活动）、连续3周（月）登录、GMV、复购率、复购率排行、点赞、评论、收藏、领优惠卷人数、使用优惠卷人数、沉默、值不值得买、退款人数、退款率  topn  热门商品
+ 订单表跟订单详情表：订单表的订单状态会变化，订单详情表不会，因为没有订单状态
+ 埋点行为数据基本格式：页面数据、事件数据、曝光数据、启动数据和错误数据。
+ 事实表
  - 事务型事实表：对应具体的事件
  - 周期型事实表：对应周期性事件（eg：月销售额）
  - 累积型事实表：对应累计事件（eg：累计销售额）
+ 数据模型：
  - 星型模型：维度表、事实表，事实表与维度表通过主键关联。
  - 雪花模型：星型模型基础上，维度表再关联维度表。
  - 星座模型：星型模型基础上，多个事实表关联同一个维度表。
+ 拉链表处理的业务场景：主要处理缓慢变化维的业务场景。（用户表、订单表）
+ 数仓中使用的哪种文件存储格式
  - 常用的包括：textFile，ORC，Parquet，一般企业里使用ORC或者Parquet，因为是列式存储，且压缩比非常高，所以相比于textFile，查询速度快，占用硬盘空间少
+ 哪张表数据量最大，是多少
  - 用户行为数据：100g(1亿条)/5 = 2千万  * 2-3倍 动作、曝光、页面、故障、启动
  - 业务数据：10万人下单，详情（50-100万条） -》加购-》下单-》支付-》物流
+ 数仓当中数据多久删除一次
  - 部分公司永久不删
  - 有一年、两年“删除”一次的，这里面说的删除是，先将超时数据压缩下载到单独安装的磁盘上。然后删除集群上数据。 很少有公司不备份数据，直接删除的。
+ 如何保证写的sql正确性（重点）：先在mysql的业务库里面把结果计算出来；在给你在ads层计算的结果进行比较、离线数据和实时数据分析的结果比较、需要造一些特定的测试数据从生产环境抓取一部分数据，测试
+ 版本号管理：5.1.2：5重大升级、1核心模块变动、2功能模块变动
+ [电商数仓表](F:\Word-Markdown\Markdown-GitHub\图片\电商数仓表.png)
+ 10.5.2 动态拆分维度表功能
  - 实时数仓动态拆分维度表功能:若用流保存维度表，维度退化需要双流join，当维度表过大会大量消耗内存资源，同时考虑高并发MySQL不合适。故采用KV型支持海量数据读写的habase，hbase支持有K快速定位数据
  - 所有维度都存在Kafka的topic_db主题，不同的维度表上下游、字段信息不同，可以把这些信息记录在配置文件中，在程序启动时加载，根据配置信息完成维度数据的处理。(采用MySQL+flinkCDC)
+ 项目中用 Flink 实时同步电商订单数据至数仓 DWD 层，任务上线后频繁重启（平均每 2 小时重启 1 次），导致数仓数据延迟超 1 小时，影响业务大屏实时展示，客户投诉较多。
  2.	排查与解决过程：
  -	第一步：定位故障原因（初期走了弯路）。
  最初认为是 “Flink 集群资源不足”，增加了 TaskManager 内存（从 4GB 增至 8GB），但重启问题仍未解决；后来查看 Flink 任务日志，发现 “Checkpoint 失败” 报错（“Checkpoint expired before completing”），才意识到核心原因是 Checkpoint 超时。
  -	第二步：分析 Checkpoint 超时根源。
  通过 Flink UI 查看 Checkpoint 详情，发现 “State Size”（状态大小）超 50GB，而 Checkpoint 超时时间仅设置为 5 分钟，状态快照无法在超时前完成；进一步分析状态大的原因：订单数据的 “窗口聚合” 设置为 “1 小时滚动窗口”，且保留了窗口内的所有明细数据，导致状态累积过大。
  -	第三步：针对性优化。
    - 调整窗口策略：将 “1 小时滚动窗口” 改为 “10 分钟滚动窗口 + 30 分钟滑动窗口”，减少单个窗口的明细数据量；②开启状态过期清理：设置窗口状态在窗口关闭后 5 分钟自动清理，避免状态累积；③延长 Checkpoint 超时时间至 15 分钟，同时增大 Checkpoint 并行度（从 1 增至 3），加快快照生成速度。
    -	优化效果：任务重启频率从每 2 小时 1 次降至每月≤1 次，数仓数据延迟控制在 5 分钟内，满足业务需求。
  -	反思与经验教训：
    -	教训 1：排查问题需 “先看日志，再下结论”，避免凭经验主观判断（初期误判为资源问题，浪费 2 天时间）。后续遇到问题，第一时间查看组件日志（如 Flink 的 taskmanager.log、Spark 的 yarn-client.log），从日志中找关键报错信息。
    -	教训 2：技术选型需 “提前评估状态管理”，尤其是实时计算场景。后续设计 Flink 任务时，会提前估算状态大小（如按 “数据量 × 窗口时长” 估算），选择合适的窗口策略和状态清理机制，避免上线后出现状态膨胀问题。
    -	教训 3：需建立 “任务监控预警机制”。后续项目中，会提前设置 Checkpoint 失败告警（如连续 2 次 Checkpoint 失败则触发告警），在问题影响扩大前及时介入，减少业务损失。

## 六个常见问题
+ 你的离职原因是什么？
  - 不说前东家坏话，哪怕被伤过，合情合理合法，不要说超过1个以上的原因
+ 您对薪资的期望是多少？
  - 非终面不深谈薪资，只说区间，不说具体数字，底线是不低于当前薪资，非要具体数字，区间取中间值，或者当前薪资的+20%
+ 您还有什么想问的问题？
  - 这是体现个人眼界和层次的问题。问题本身不在于面试官想得到什么样的答案，而在于你跟别的应聘者的对比
  - 标准答案：公司希望我入职后的3-6个月内，给公司解决什么样的问题
  公司（或者对这个部门）未来的战略规划是什么样子的？以你现在对我的了解，您觉得我需要多长时间融入公司？
+ 您最快多长时间能入职？一周左右，如果公司需要，可以适当提前。
+ 自我介绍（控制在4分半以内，不超过5分钟）
  个人基本信息
  工作履历：时间、公司名称、任职岗位、主要工作内容、工作业绩、离职原因
  深度沟通（也叫压力面试）
  刨根问底下沉式追问（注意是下沉式，而不是发散式的）
  基本技巧：往自己熟悉的方向说


































