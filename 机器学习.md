[toc]
***

# 机器学习
+ 定义：计算机程序针对特定任务，通过经验学习自动改进性能。
+ 机器学习是人工智能的基础，深度学习是机器学习的一种。
+ 分类：
  - 监督学习：有标签：对于输入数据X可预测Y
    eg：回归、K临近、SVM、决策树、朴素贝叶斯、逻辑回归
  - 无监督学习：无标签：对于输入数据X能发现什么
    eg：聚类、PCA（降维算法）、EM算法
  - 强化学习：有奖励机制，序列决策问题。
    eg:马尔科夫决策算法
+ 开发一般步骤：
  - 数据采集和标记：采集训练样本，并对其进行标记。
  - 数据清洗：数据进行预处理，使得模型可直接使用。
  - 特征选择
  - 模型选择
  - 模型训练和测试
  - 模型性能评估和优化
+ conda配置镜像：
  - 国内镜像无版本使用国外镜像，通过-c参数，并连接手机热点，电脑开VPN可加快下载速度。
  - PyCharm环境可见myenv但是选择后不生效,而默认环境可生效->权限不足，以管理员身份运行PyCharm
  - 环境变量配置：
    ```
    D:\Anconda
    D:\Anconda\Scripts
    D:\Anconda\Library\bin
    ```
  - cmd
    ```
    conda config --remove-key channels
    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
    conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
    conda config --set show_channel_urls yes
    ```
  - 或修改C:/Users/杨雨糯/.condarc
    ```
    channels:
      - defaults
    show_channel_urls: true
    default_channels:
      - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
      - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
      - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
    custom_channels:
      conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
      msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
      bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
      menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
      pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
      pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
      simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
    ```
+ 高版本conda不能被PyCharm自动识别：添加SDK手动找到D:\Anconda\condabin\conda.bat添加进来。
## 基础概念
+ 过拟合:是指模型能很好地拟合训练样本 ，但对新数据的预测准确性很差 。
  - 减少输入特征数量。
  - 获取更多训练数据。
  - 正则化：在损失函数中添加正则项，使模型参数变小，从而降低过拟合的风险。
+ 欠拟合:是指模型不能很好地拟合训练样本 ，且对新数据的预测准确性也不好
  - 增加输入特征数量。
  - **为什么增加多项式特征可以优线性回归模型准确性**？答：通过增加多项式特征，实际上是增加了模型的复杂度。许多实际数据的分布是复杂的、非线性的。增加多项式特征可以让模型更好地适应这种复杂的数据分布
  - 增加特征项。
+ 成本 :是针对所有训练样本 ，模型拟合出的值与训练样本的真实值的 误差平均值 。成本是衡量模型与训练样本符合程度的指标 。
+ 成本函数: 是成本与模型参数的函数关系
+ 梯度下降：随机选择一组参数，一个学习率a(移动步长)，按梯度方向移动一步，多次迭代后求得更小的成本函数值。
+ 模型准确性：成本函数越小，模型准确性越高。(按8:2/7:3划分训练集和测试集，
  将数据集划分为训练集和测试集，训练集用于训练模型，测试集用于评估模型准确性。)
  - 交叉验证：数据集划分为训练集、交叉验证集、测试推荐6:2：2,以不同参数在训练集
    上训练，在交叉验证集上评估，选出最优参数，最后在测试集上评估。
+ 学习曲线：以训练集大小为横坐标，以模型准确性为纵坐标，绘制出的曲线。用于观察模型准确性与训练集大小的关系。
+ 查准率: 是模型预测为正例的样本中，实际为正例的比例。
+ 召回率: 是实际为正例的样本中，模型预测为负例的比例。
+ scikit-learn 广播机制：两个矩阵运算，维度不匹配时，将小维度扩展为与另一个矩阵匹配的维度。
## 工具
### IPython
+ jupyter notebook//启动jupyter notebook
+ import numpy as np//导入n科学计算库
+ %matplotlib inline//设置inine方式在网页上显示图像
+ import matplotlib.pyplot as plt//导入可视化工具库
### numpy使用见数据可视化
```
import numpy as np
import matplotlib.pyplot as plt
```
### pandas数据处理工具包

## KNN （有监督的机器学习算法）
+ KNN（K-Nearest Neighbors），即K近邻算法，是一种基本的分类和回归算法：
### 基本原理
+ **核心思想**：对于一个新的数据点，在训练数据集中找到与它最相似的K个邻居，然后根据这K个邻居的类别来预测新数据点的类别，或根据数值预测数值。
+ **距离度量**：通常使用欧氏距离、曼哈顿距离、闵可夫斯基距离等来衡量数据点之间的相似性。例如，欧氏距离的计算公式为：\(d(x,y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}\)，其中\(x=(x_1,x_2,\cdots,x_n)\)和\(y=(y_1,y_2,\cdots,y_n)\)是两个数据点，\(n\)是数据的维度。
+ **K值**：
  -  K值越小，模型方差越大可能导致模型过拟合，
  -  K值越大，模型偏差越大,可能使模型过于平滑而欠拟合。
+ **成本函数**：
### 特点
- **优点**：
    - 高准确性，对异常值和噪声容忍高。
- **缺点**：
    - 计算量大，依赖内存。
### 应用场景
- **分类问题**：如手写数字识别、图像分类、文本分类等。在手写数字识别中，将手写数字的图像转换为特征向量，然后使用KNN算法根据训练集中的已知数字图像来识别新的手写数字图像所属的类别。
- **回归问题**：如房价预测、股票价格预测、糖尿病检测等。在房价预测中，根据房屋的面积、位置、房龄等特征，使用KNN算法预测新房屋的价格。
- **推荐系统**：可以根据用户的历史行为数据，如购买记录、浏览记录等，找到与目标用户最相似的K个用户，然后根据这些相似用户的喜好来为目标用户推荐商品或服务。
### **KNN 变种**
+ 加权KNN:为每一个临近点赋予一个权重，权重与距离成反比。
+ 基于距离KNN:使用在半径R内的所有点取代距离最近的K个点。
+ 自适应KNN:根据数据集的特性，动态调整K值。
+ 多尺度KNN:使用多个不同尺度的KNN模型，然后综合这些模型的预测结果。
### 聚类算法性能评估
+ 齐次性/完整性：同一类型数据是否在同一类中。
+ 轮廓系数（可评估无监督学习）：($s=\frac{b-a}{max(a,b)}$),其中a为样本i到同类中其他样本点的平均距离，b为样本i到距离最近下一个类中样本点的平均距离。
### 实现步骤
- **数据准备**：收集和整理训练数据集，包括特征向量和对应的标签（分类任务）或目标值（回归任务）。
- **确定K值**：选择合适的K值，一般通过交叉验证等方法来确定。K值的选择会影响模型的性能，较小的K值可能导致模型过拟合，较大的K值可能使模型过于平滑而欠拟合。
- **计算距离**：对于待预测的数据点，计算它与训练数据集中每个数据点的距离。
- **选择邻居**：根据计算出的距离，找出距离最近的K个邻居。
- **预测结果**：
- **分类任务**：根据这K个邻居中多数的类别来决定待预测数据点的类别，通常采用投票法，即选择出现次数最多的类别作为预测结果。
- **回归任务**：计算这K个邻居的目标值的平均值或加权平均值作为待预测数据点的预测值，权重可以根据距离的远近进行设置，距离越近权重越大。
### **k-均值算法与k-近邻算法的区别**
+	目的不同
  - k均值算法：是一种无监督学习算法，主要用于聚类分析，将数据划分为不同的簇，发现数据的内在分布结构，
    使得同一簇内的数据点尽可能相似，不同簇之间的数据点尽可能不同，簇的划分是根据数据点到簇中心的距离。
  - k近邻算法：是一种监督学习算法，主要用于分类和回归任务（通常用于分类）。
    它根据训练数据集中与新样本最近的个邻居的类别来预测新样本的类别（分类时）
    或通过邻居的数值来估计新样本的数值（回归时）。
+	算法过程不同
  - k均值算法：重点在于不断更新簇中心和重新分配数据点到簇中，
    通过迭代优化来确定最终的个簇。在计算距离时，是计算数据点到簇中心的距离。
  - k - 近邻算法：在预测时，计算新样本到训练数据集中所有样本的距离，找到最近的个邻居，
    然后根据这个邻居的信息（如多数邻居的类别等）来做出预测。
    在训练阶段，k - 近邻算法只是简单地存储训练数据，没有复杂的训练过程。
+ 输出结果不同
- k - 均值算法：输出是个簇的划分，以及每个簇的质心等信息，它描述了数据的聚类结构。
- k - 近邻算法：输出是对新样本的预测类别（分类任务）或预测数值（回归任务），是一个具体的预测值。


## 线性回归算法
+ 是一种广泛应用于机器学习领域的监督学习算法，用于建立自变量和因变量之间的线性关系模型，如用来预测房价。
### 基本原理
+ 线性回归基于这样一个假设：自变量 \(x\) 和因变量 \(h(\theta)\) 之间存在线性关系，其**预测函数**为 \(h(\theta) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n + \theta\)，其中 \(h(\theta)\) 是因变量，\(x_1,x_2,\cdots,x_n\) 是自变量，\(\theta_0,\theta_1,\cdots,\theta_n\) 是模型的参数，当仅有一个含X项时，线性回归模型称为单变量线性回归。
  - **预测函数矩阵形式**：\(h(\theta) = [\theta_0,\theta_1,\cdots,\theta_n] \cdot [x_0,x_1,\cdots,x_n]^T=X\theta^T\)，其中 \(X\) 是自变量矩阵转置，\(\theta\) 是参数向量矩阵，\(\theta\) 是误差向量。
+ 成本函数(衡量预测效果)：$j(\theta)=\frac{1}{2m}\sum_{i=1}^{m} (h(x_i)-y_i)^2$，矩阵形式：$J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)$,矩阵形式无需累加器，矩阵运算效率高。
+ 梯度下降的公式为 \(\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}\) ，其中 \(\alpha\) 是学习率，\(J(\theta)\) 是目标函数，\(\frac{\partial J(\theta)}{\partial \theta_j}\) 是目标函数对参数 \(\theta_j\) 的偏导数。 **\(\alpha\) 学习率：过大可能越过极小值点，模型无法收敛；过小可能导致收敛速度过慢。**
+ 梯度下降的参数迭代公式：将成本函数代入梯度下降的公式结果为：$$\theta_j := \theta_j - \frac{\alpha }{m} \sum_{i=1}^{m} (h(x_i)-y_i)x_j$$
### 附加操作
+ 数据归一化：将数据缩放到一个较小的范围内，如[0,1]或[-1,1]，可提高计算效率，加快收敛速度，但**对模型准确性无影响**
+ 增加特征项：实际数据为非线性数据，通过增加特征项，提高拟合效果。
+ 梯度下载算法目的：通过迭代求得更佳的$\theta$，使成本函数最小化,$\theta$移动的方向由偏导数决定。
### 优缺点
+ 线性回归假设自变量和因变量之间存在线性关系，如果实际关系是非线性的，线性回归模型的拟合效果可能会很差。
+ 对异常值比较敏感，异常值可能会对模型的参数估计和预测结果产生较大的影响。
+ 可能存在多重共线性问题，即自变量之间存在高度相关性，这可能会导致模型的参数估计不稳定，影响模型的预测性能。

## 逻辑回归算法（解决分类问题）
### 基本原理
+ 逻辑回归主要用于分类问题，它假设输入特征 \(x=(x_1,x_2,\cdots,x_n)\) 与样本属于某一类别的概率 \(p(y=1|x)\) 之间存在一种映射关系。其预测函数通过引入一个非线性的激活函数（通常为Sigmoid函数）来构建。
+ 预测函数表示为 \(h_{\theta}(x) = \frac{1}{1 + e^{-\theta_0 - \theta_1x_1 - \theta_2x_2 - \cdots - \theta_nx_n}}\)，这里\(\theta = (\theta_0,\theta_1,\cdots,\theta_n)\) 是模型的参数，\(h_{\theta}(x)\) 可以解释为给定输入特征 \(x\) 时，样本属于正类（比如 \(y = 1\) 所代表的类别）的概率，即 \(P(y = 1|x)=h_{\theta}(x)\)，那么样本属于负类（\(y = 0\) ）的概率就是 \(P(y = 0|x)=1 - h_{\theta}(x)\)。
+ **预测函数矩阵形式**：\(h_{\theta}(x) = \frac{1}{1 + e^{-\theta^TX}}\)

+ 逻辑回归成本函数（衡量预测效果）：\(J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y_i\log(h_{\theta}(x_i))+(1 - y_i)\log(1 - h_{\theta}(x_i))]\)
  - 这里 \(m\) 是训练样本的数量，其目的是衡量模型预测概率与真实类别标签之间的差异，通过最小化这个成本函数来训练模型找到最优的参数 \(\theta\) 。
  - 逻辑回归常用的成本函数是对数似然损失函数，对于单个样本 \((x_i,y_i)\) ，其成本函数为：
  \[
  \begin{cases}
  -\log(h_{\theta}(x_i)) & \text{if } y_i = 1 \\
  -\log(1 - h_{\theta}(x_i)) & \text{if } y_i = 0
  \end{cases}
  \]
  - **与线性回归成本函数对比**：不同于线性回归的成本函数 \(j(\theta)=\frac{1}{2m}\sum_{i=1}^{m} (h(x_i)-y_i)^2\) ，逻辑回归的成本函数基于概率和对数似然的概念构建，更符合分类问题中对概率估计准确性的衡量需求，而线性回归的成本函数侧重于衡量预测值与真实值差值的平方和。
+ 梯度下降
  - 同样采用梯度下降算法：\(\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}\) ，其中 \(\alpha\) 是学习率，\(J(\theta)\) 是成本函数，\(\frac{\partial J(\theta)}{\partial \theta_j}\) 是成本函数对参数 \(\theta_j\) 的偏导数。
+ **梯度下降的参数迭代公式推导（以逻辑回归为例）**：经过一系列的求导运算（基于链式法则等对成本函数 \(J(\theta)\) 求关于 \(\theta_j\) 的偏导数），将成本函数代入梯度下降的公式结果为：
  \[
  \theta_j := \theta_j - \frac{\alpha }{m} \sum_{i=1}^{m} (h_{\theta}(x^i)-y^i)x_j^{(i)}
  \]
+ 过拟合解决
  - 正则化：解决模型过拟合，保留所有的特征减少特征权重，
    操作为预测函数中加入正则项，正则参数($\lambda$)越小拟合效果越好
  - 减少输入特征
  - 获取更多训练数据
+ 正则项：$\frac{1}{2m} \sum_{i=1}^{m} \theta^2_k$,再逻辑回归模型加上正则项即完成模型正则化。正则化后模型的梯度下降的参数迭代公式：$\theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \frac{\alpha }{m} \sum_{i=1}^{m} (h_{\theta}(x^i)-y^i)x_j^{(i)}$，其中$\lambda$为正则化系数，$\lambda$越大，正则化越强，$\lambda$越小，正则化越弱，拟合效果越好。
+ **L1|L2正则项**：L1范数：$||\theta||_1=\sum_{i=1}^{n}|\theta_i|$，L2范数：$||\theta||_2 = \sqrt{\sum_{i=1}^{n}\theta_i^2}$
+ 最大似然估计: 通过现实世界统计概率，来估计模型参数，使模型预测结果与实际结果最接近。（eg：理论硬币概率0.5:0.5,但在N次统计时概率为0.3:0.7出现的概率最大，则模型参数选择为0.3:0.7）
  - 最大是然估计值可用于构建逻辑回归成本函数。
  - 对数几率回归可以看作是逻辑回归的一种特殊情况，其中因变量 \(y\) 只有两个取值（0和1），且因变量 \(y\) 的取值与自变量 \(x\) 之间存在线性关系。
# 应用场景与问题
+ 乳腺癌预测
+ 怎样理解正则化后的成本函数解决了过拟合问题？
  - 从数学角度看，增加了正则项$\frac{\lambda}{2m}\sum_{k = 1}^{n}\theta_{k}^{2}$后，成本函数不再唯一地由预测值与真实值的误差所决定，还和参数$\theta$的大小有关。
  - 有了这个限制之后，要实现成本函数最小的目的，$\theta$就不能随便取值了。
  - 比如某个比较大的$\theta$值可能会让预测值与真实值的误差$(h_{\theta}(x)-y)^{2}$值很小，但会导致$\theta$很大，最终的结果是成本函数太大。这样，通过调节参数$\lambda$，就可以控制正则项的权重，从而避免线性回归算法过拟合。

## 决策树算法（解决分类问题）
### 基本原理
+ 决策树是一种树形结构，每个分支节点代表一个特征判断，根据判断结果进行分类，每个叶节点代表一种类别。
+ 信息熵：香农提出信息熵（信息不确定性越高信息熵越高），解决信息量化问题。**信息熵公式**：$H(x)=-\sum_{x\subset X}^{}P(x)log_2P(x)$，其中P(X)为X发生的概率，单位比特
+ 信息增益：信息增益是用来衡量数据变得更有序、更纯净的程度的指标
+ 构建决策树时，应该优先选择哪个特征来划分数据集答案是：遍历所有的特征，分别计算，得到每个特征划分数据集前后信息的变化值，选择信息熵变化幅度最大的那个特征，优先作为数据集划分依据。即选择**信息增益**最大的特征作为分裂节点。
+ 构建结束标志：
  - 可用特征集为空
  - 继续划分提升信息增益小于设定值。
### 决策树创建步骤：
+ 计算数据集划分前的信息熵；
+ 遍历所有未作为划分条件的特征，分别计算根据每个特征划分数据集后的信息熵；
+ 选择信息增益最大的特征，并使用这个特征作为数据划分节点划分数据；
+ 递归地处理被划分后的所有子数据集，从未被选择的特征里继续选择最优数据划分特征来划分子数据集。
### 决策树过拟合解决：
+ 离散化：连续属性值
  - 将连续属性值划分为多个区间，每个区间取一个值，然后按照离散属性值（信息增益阈值）的方法进行划分。
+ 正则化：优先选择取值较多的特征：
  - 计算信息熵时加上一个与类别个数成正比的正则项。
+ 前剪枝：在决策树构建过程中，通过设置最大深度、最小样本数、最大叶节点数等参数来限制树的深度，从而避免过拟合。
+ 后剪枝：在决策树构建完成后，通过比较子分支是否有继续分裂的必要来删除一些不必要的分支，从而简化决策树的结构，提高模型的泛化能力。
### ID3算法
+ ID3算法使用信息增益作为特征选择的标准，选择信息增益最大的特征作为分裂节点，递归地构建决策树。但倾向于选择取值较多的特征，可能导致过拟合。
+ **解决优先选择取值较多的特征**：计算信息熵时加上一个与类别个数成正比的正则项。
### CART 算法：
+ 采用基尼指数作为特征选择的标准，可用于分类和回归任务。在分类任务中，基尼指数越小，说明该特征对分类的纯度提升越大；在回归任务中，通过最小化平方误差来选择最优的划分特征和划分点。CART 算法生成的决策树是二叉树，结构相对简单，易于理解和实现。
+ **解决部分属性值连续**：将其划分为多个区间，每个区间取一个值。实现离散化。
+ 基尼指数：衡量信息不纯度指标，越接近0纯度越高。**基尼指数公式**：$Gini(D)=1-\sum_{k=1}^{K}p_k^2$，其中K为类别个数，$P_k$为样本属于第k类的概率。


## 朴素贝叶斯算法（解决分类问题）
### 基本原理
- **贝叶斯定理**：在分类问题中，假设我们有类别\(Y\)和特征向量\(X=(X_1,X_2,\cdots,X_n)\)，贝叶斯定理表示为\(P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}\)。
- **“朴素”**：朴素贝叶斯算法**假设特征\(X_1,X_2,\cdots,X_n\)在给定类别\(Y\)的条件下是相互独立的**。这意味着\(P(X|Y)=P(X_1|Y)P(X_2|Y)\cdots P(X_n|Y)\)。基于这个假设，朴素贝叶斯分类器可以简化计算，大大提高了计算效率。
- 在朴素贝叶斯中，假设特征之间相互独立（这是一个较强的假设），则P(x|y)=P(x1|y)*P(x2|y)…*P(xi|y)。
  通过训练数据估计出P(y)（各类别的先验概率）和xi（每个特征在各类别下的条件概率）
  然后对于新样本x，计算每个类别对应的P(x,|y)P(y)，取其最大值对应的类别作为预测类别。
### 工作流程
+ **准备数据**：收集和整理带有类别标签的训练数据，每个样本包含一组特征和对应的类别。
+ **计算先验概率**：对于每个类别\(C\)，计算其在训练数据中出现的概率\(P(C)\)，即类别\(C\)的样本数除以总样本数。
+ **计算类条件概率**：对于每个特征\(X_i\)和类别\(C\)，计算在类别\(C\)中特征\(X_i\)出现的概率\(P(X_i|C)\)。通常可以通过统计在类别\(C\)的样本中特征\(X_i\)出现的频率来估计。
+ **预测分类**：对于一个新的样本\(X=(X_1,X_2,\cdots,X_n)\)，计算它属于每个类别\(C\)的后验概率\(P(C|X)\)。根据贝叶斯定理和特征条件独立假设，\(P(C|X)=\frac{P(X|C)P(C)}{P(X)}=\frac{P(X_1|C)P(X_2|C)\cdots P(X_n|C)P(C)}{P(X)}\)。由于\(P(X)\)对于所有类别都是相同的，所以在比较不同类别的后验概率时可以忽略不计。因此，只需要计算\(P(X_1|C)P(X_2|C)\cdots P(X_n|C)P(C)\)，并选择概率最大的类别作为预测结果。

### 优点
+ **算法简单高效**：基于贝叶斯定理和特征独立假设，计算过程相对简单，易于理解和实现，在大规模数据集上训练和预测速度快。
+ **对小规模数据表现良好**：在数据量较小的情况下，朴素贝叶斯仍然能够取得较好的分类效果，尤其适用于文本分类、垃圾邮件过滤等领域。
+ **对缺失数据不敏感**：在实际应用中，数据常常存在缺失值，朴素贝叶斯算法在处理缺失数据时相对简单，不需要对缺失值进行复杂的填充或处理。

### 缺点
+ **特征独立性假设限制**：在实际应用中，特征之间往往存在一定的相关性，这可能导致朴素贝叶斯算法的分类性能下降。
+ **对输入数据的表达形式敏感**：在文本分类中，如果文本的表示方式发生变化，例如词袋模型中的词汇选择、词频统计方法等，可能会对分类结果产生较大影响。

### 应用场景
+ **文本分类**：如新闻分类、情感分析、垃圾邮件过滤等，将文本划分为不同的类别，如政治、经济、娱乐等，或判断文本的情感倾向是积极、消极还是中性，以及识别垃圾邮件和正常邮件。
+ **疾病诊断**：根据患者的症状、检查结果等特征，判断患者可能患有的疾病。例如，根据咳嗽、发热、流涕等症状，判断患者是感冒、流感还是其他疾病。
+ **推荐系统**：基于用户的历史行为和偏好，预测用户对不同物品的喜好程度，从而进行个性化推荐。例如，根据用户对电影的评分、观看历史等，推荐其他可能感兴趣的电影。

##  PCA 算法（主成分分析算法）：
+ PCA 是一种常用的数据降维算法。
  其主要思想是将高维数据投影到低维空间，同时尽可能地保留数据的方差信息（即数据的分布特征）。
  通过计算数据的协方差矩阵，求其特征值和特征向量，
  选择较大特征值对应的特征向量组成投影矩阵，将原始数据投影到新的低维空间。
###	PCA算法作用
+	数据可视化：当数据维度很高时，无法直接在二维或三维空间中可视化。
  PCA 可以将高维数据降到二维或三维，从而可以绘制数据点，直观地观察数据的分布模式
  、聚类情况等。例如在分析一个有多个特征（如年龄、收入、消费习惯等多个维度）
  的客户数据集时，通过 PCA 降到二维或三维后在平面或空间中绘制客户点，观察客户群体的分布特征。
+	降低计算复杂度：在很多机器学习算法中，计算复杂度随着数据维度的增加而急剧增加。
  PCA 降维后，数据维度降低，在后续的数据分析、模型训练等过程中可以减少计算量和存储空间。
  例如在处理图像数据（图像像素点多，维度高）时，先进行 PCA 降维再进行分类或聚类等操作，可以提高计算效率。
+	去除噪声和冗余信息：高维数据中可能存在一些冗余特征或噪声信息，
  PCA 在降维过程中，通过保留主要成分（方差较大的方向），
  可以在一定程度上过滤掉这些噪声和冗余信息，提高数据质量，
  使得后续基于降维后数据的分析和模型更加准确和稳定。
### PCA算法的数据还原率
+ 数据还原率表示通过 PCA 降维后的数据再还原回原始空间时，能够恢复原始数据信息的程度。
  反映了 PCA 算法在降维过程中对原始数据信息的保留情况，
  如果还原率高，说明降维过程中丢失的信息较少；
  反之，如果还原率低，则丢失的信息较多。
+ 数据还原率计算方法：
  - **计算原理**：在PCA中，首先将原始数据进行中心化处理，然后通过计算协方差矩阵的特征值和特征向量，选择前\(k\)个最大特征值对应的特征向量构成投影矩阵，将原始数据投影到低维空间。
    在还原数据时，将低维数据通过投影矩阵的逆变换或伪逆变换映射回原始高维空间，得到重构数据。数据还原率可以通过计算原始数据与重构数据之间的误差来衡量，误差越小，还原率越高。
  - **计算公式**：假设原始数据矩阵为\(X\)，经过PCA降维后再还原得到的重构数据矩阵为\(\hat{X}\)，则重构误差\(E\)可以用均方误差（MSE）来表示，
    计算公式为\(E = \frac{\frac{1}{m}\sum_{i = 1}^{m}||x_i - \hat{x}_i||^2}{\frac{1}{m}\sum_{i = 1}^{m}||x_i||^2}\)，其中\(n\)是数据点的数量，\(x_i\)是原始数据中的第\(i\)个数据点，\(\hat{x}_i\)是重构数据中的第\i个数据点。数据还原率\(R\)可以定义为\(R = 1 - \frac{E}{E_{max}}\)，其中\(E_{max}\)是当重构数据为随机噪声或零矩阵时的最大可能误差，通常可以用原始数据的方差来近似表示，即\(E_{max}=\frac{1}{n}\sum_{i = 1}^{n}||x_i - \bar{x}||^2\)，其中\(\bar{x}\)是原始数据的均值向量。

## 支持向量机（SVM）：二元分类问题
+ SVM：按监督学习方式对数据进行二元分类的广义分类器，
  其决策边界是对样本学习的求解的最大边距平面。
+ SVM 原理：找到一个分割平面($w^Tx+b=0$)，其可正确分类且间距最大。（类似一条线分割红黑双方多个点）
+ （软）硬间隔：（不）允许分类出现个别分类错误。
+ SVM 超参：C（惩罚系数，控制分类错误的惩罚程度，C 越大，对分类错误的惩罚越重，拟合越高）。$\gamma$（$\gamma$ 越大，支持向量越少，分类边界越接近一次函数，可能过拟合）

## 感知机:二分类线性可分问题
+ 定义：当输入多个信号（$x_1,...,x_n$）,信号乘上固定的权重（$w_1,...,w_n$）并求和，然后加上一个偏置（$b$）传入$h(x)$得到输出（$y,x>0->1,x<0->1$）,（$y=h(\sum_{i=1}^{n}w_ix_i+b)$），又称为单层神经网络。
+ 参数：$b$（偏置），-b称为阈值，反映了神经元的激活难度。
+ $y=h(\sum_{i=1}^{n}w_ix_i+b)$，称为突变函数，超过了一定阈值，输出为1，否则为0。
+ 局限性：不能实现异或门，只能解决线性可分问题。
+ 多层感知机：感知机可以堆叠多层，解决非线性问题。
+ 可解决分类问题和回归问题。

## 神经网络
+ 定义：由多个神经元（感知机）组成输入层、中间层（隐藏层）、输出层组成的网络。
+ 激活函数（连接感知机和神经网络的桥梁）：$y=h(\sum_{i=1}^{n}w_ix_i+b)$，$h(x)$称为激活函数，用于将神经元的输入信号转换为输出信号，$h(x)=\frac{1}{1+e^{-x}}$，$h(x)=y$输出为0~1间连续的值，称为sigmoid函数，故神经网络中流动的是连续的实数信号。
  - sigmoid为非线性函数，若h(x)为线性函数，则多层神经网络可简化为两层，神经网络为线性模型，无法解决非线性问题。
+ 神经网络中信号传递：信号从输入层传递到输出层，信号经过隐藏层时，信号会经过激活函数（$f(x)$）进行非线性变换，再传递到下一层。
+ 隐藏层可神经元数可多于输入层，但之后的隐藏层神经元数一般比前一层少，做一个特征压缩。
+ 损失函数引入：
  - 不采用识别精度为指标：因识别精度对微小的参数变化不敏感，无法通过梯度下降法优化参数。
  - 不使用跃迁函数为激活函数：因其对于微小的参数变化不敏感，无法通过梯度下降法优化参数。
  - 损失函数，用于衡量模型预测结果与真实结果之间的差异。
+ 交叉熵误差：部分样本同时属于两个类别，($E=\sum_{j=1}^{m}y_jlog(y_j)$)，$y_j$为样本属于第$j$类别的概率，$m$为样本数量。
+ 多分类问题：采用softmax：$softmax(y_k)=\frac{e^{x_k}}{\sum_{k=1}^{n}e^{x_k}}$，其中$k$为类别数，$y_k$为输出层第$k$类别的神经元输出值，$x_k$为输出层第K个神经元输入信号，n为输出层神经元个数。
  - $softmax(x_k)$为第$k$类别的输出概率，$softmax(x_k)$的输出值之和为1故可将其输出称为概率，可以用于多分类问题。将输出层神经元的输出值转换为概率分布，使得所有输出值之和为1，可以用于多分类问题。
+ SGD:随机梯度下降算法，通过随机选择数据点，计算梯度，更新参数，迭代优化模型。
+ 反向传播（**用于自动计算梯度**）：（正向传播存在大量计算，但已知结果和计算步骤，可反推出计算过程中各个步骤的中间结果，进而得到参数）计算损失函数对每个参数的梯度，用于更新参数，优化模型。










