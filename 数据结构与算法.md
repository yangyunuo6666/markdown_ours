[toc]
***

# 算法
## 常用思路
### 常用解题：
+ 判断二叉树：
  - 满二叉：除最后一层外，每一层都是满的，且最后一层的叶子节点靠左排列。（层次遍历，null只会在最后出现）
  - 完全二叉树：左右子树相同，且孩子数为2。（）
### 极限思维：考虑极端情况如何处理，极限 + 一般 = 所有
+ 输入的是一个空数组、空指针
### 指针：
+ 快慢指针：
  - 快指针走两步，慢指针走一步，fast = null时slow指向中间。(类似的可以找到1/3、1/4等位置)
+ 相撞指针：
  - 中位查找：两个指针分别从两端开始，向中间移动，直到相遇。
  - 环判断：判断链表是否有环，相遇时说明有环。
+ 空值头节点：需要对链表头节点操作时，new LinkedNode(0).next = head;
+ 创建节点是不一定需要保存节点指针：head.next = new LinkedNode(val);
### 递归：
  - 查找极值：不断分裂直到一组为2个元素，比较大小后向上递归合并。
  - 峰值查询：mid = (left+right)/2，比较mid与mid+1的大小，若mid大，则向左递归，若mid+1大，则向右递归。
  - **递归调用的多个返回值**：return merge() && merge()，返回两个结果,return merge() || merge()，返回一个结果。
## 算法基础
1.	时间复杂度：数据趋于N时，所需的时间。Eg：两层for循环复杂度为N*N，一层for循环i+=2复杂度为log2 N（如二分搜索，一亿个数据仅需27次）
2.	空间复杂度：数据趋于N时，所需的时间。（有的算法需要额外的辅助空间）
 
## 分治法
+ 将大问题分为小问题解决后合并最终解决大问题，eg：有序数组的二分查找。
+	步骤：
  - 分解问题为子问题，注意停止分解条件
  - 解决子问题，并合并结果，向上递归
  - 到达终止条件，返回结果

## 动态规划
+	动态规划减少了重复的计算，但是要额外的存储空间。
  - 主要用在：**解决重叠子、最优子问题。**
+	重叠子问题：即在求解原问题的过程中，我们需要解决很多**重复的子问题**。因此，我们可以使用动态规划算法将这些子问题解决一次，然后将它们的解缓存起来，以节省时间。
+	最优子结构性质：原问题的最优解可以通过求解子问题的最优解来获得。
  - 因此，我们可以使用动态规划算法将原问题的最优解分解为若干个子问题的最优解，然后通过这些最优解来构造原问题的最优解。
+ 常用步骤：
  1. 确定状态：将问题分解为若干个子问题，并明确每个子问题中的状态是什么。
  2. 定义状态转移方程：通过递推的方式，确定每个状态是如何由之前的状态转移而来的。
  3. 初始化：确定初始状态的值，通常是问题中的边界条件。
  4. 计算最终结果：通过状态转移方程，逐步计算得到最终的解。
  4.	状态转移方程：


 
## 排序
### 插入排序：
 +	直接插入排序
 - 将第一个元素作为有序序列，从第二个元素开始，将元素插入到有序序列中。
 - 新元素从后向前找，找到第一个比它大的元素，将新元素插入到该元素之前。
 +	二分插入：新元素插入时结合了二分查找。
 +	二路插入：将第一个元素作为中间mid，小于mid在左边折半插入。
### 冒泡排序：
 +	冒泡排序：通过“交换”无序序列中的相邻记录从而得到其中关键字最小或最大的记录，并将它加入到有序序列中以增加记录的有序序列的长度。当一次排序交换没有发生排序可提前结束。
 +	快速排序（**关键词：划分**）：取出第一个记录保存在temp中，将变量i,j从两端开始扫描，i从左端开始，j从右端开始。先从右端开始，j向左移动，找到第一个小于基准记录的记录，将i与j所指记录交换，然后i向右移动，继续上述操作。直到i==j，将temp赋值给i所指记录交换。以I划分左右区间，对左右区间分别进行上述操作。
 +	快速划分：用a[i] a[j]代表左右记录，与中枢比较后若左记录小，将其与中枢交换。直到i==j，可以暂存中枢结束后再将中枢放好。
3.	选择排序：
 +	直接选择排序：从序列中选取最小值，放到序列的起始位置。重复操作，直到序列有序。
 +	堆排序：通过优先队列。
### 归并排序：通过“归并”两个或两个以上的有序序列，逐步增加有序序列的长度。
 + 两组归并算法：
  - 从第一个元素开始将两个有序序列进行比较，将较小的元素放入辅助数组，直到一个有序序列全部放入辅助数组。
  - 将两个序列的剩余元素依次放入辅助数组。
  - 复制
  + 一路归并算法：
   - 将各个长度为len的子序列两两归并，为2*len长度的序列。
  + 二路归并算法：
   - 从len=1开始，掉用一路归并直到len=n;
###  分配排序：通过对无序序列中的记录进行反复的“分配”和“收集”操作，逐步使无序序列变为有序序列。
### 表插入排序：用静态列表保存排序记录，排序过程就是改变指针值。
### 希尔排序（**分割为子序列，子序列分割最小后插入排序，再将较大的子序列直接插入排序**）：
  + 序列长度为N，取d=N/2，d=N/2^k，d=N/2^(k+1)。
  + 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。
  + 先取d=N/2,将间隔为D的数据为一组进行直接插入排序，再取d=d/2,重复上述操作。直到d=1,即所有间隔为1的数据为一组进行直接插入排序。
    
### 堆排序：
 + 建立最大堆
 + 将根节点与最后一个节点交换，将最后一个节点删除。
 + 对堆进行调整，使其满足最大堆的定义。
 + 重复上述操作，直到堆中仅剩一个节点。
### 桶排序
  + 取合适的桶数，将待排序数组分到各个桶中，入桶时要有序。
  + 全部入桶后进行收集。
  + 进行二次分配和收集。

### 计数排序
+ 统计比当前值大的数的个数，该个数即为位置。

 
## 搜索
1.	斐波那契查找：建立斐波那契树查找。
2.	二分查找（**0必须是线性储存的数据**）：注意a>b[mid]时add=mid+1;(a[mid]已经比较过)
3.	差值查找：数据差值均匀，mid=f((add+len));
4.	分块查找（索引顺序查找）：将数据分块，以每一块的首元建立索引表（辅助查找的数组）
5.	二叉查找树：


 


# 数据结构
1. 数据结构的构成：
   +  逻辑结构：元素之间的相互关系。
		* 线性结构：除头尾节点，每一个节点有仅有一个前驱和后继。
		* 树形结构：除根节点外，每一个节点仅有一个前驱可有多个后继。
		* 图（复杂结构）：前驱和后继数目无要求。
   +  存储结构：逻辑结构在计算机中的存储方式。
		* 顺序结构：逻辑上连续的物理上也连续。
		* 连接结构：逻辑上连续的物理上可能连续。
		* 散列表示：地址转化法，如字典。
		* 索引表示：建立辅助的索引结构。
   +  操作：方法
2.	栈区：自动申请，自动释放，编译器管理，空间有限（KB-几MB）。
3.	堆区：手动申请，手动释放，自己管理，空间大。
4.	时间复杂度：当数量N趋于无穷大时，时间与N的比例关系
3.1.O(n)线性相关 O(n*n)两层for循环成平方关系 O(n*log2N)第一层for每次i递增2第二层递增1。	
5.	空间复杂度：当数量N趋于无穷大时，花费的额外的空间与N的比例关系。
+  时间复杂度与空间复杂度是相互影响的，可牺牲一个成全另一个。
+ 线性与非线性结构：
  - 线性：元素之间存在一一对应关系。常见的非线性结构有：二（N）维数组，广义表，数，图。
  

## 线性表
1.	由有限个具有相同性质的元素组成的有序序列，含大量记录的线性表保存在外存上称为文件。
+ 顺序存储的存取、查找方便，链式存储插入和删除方便。
2.	线性表的
+	数组实现：
```c
typedef struct 
{
	 int a[100];//存放线性表的数组 
	 int last;//线性表的长度 
}seqlist
```
2.2链表实现：用链表存储数据。（头插法和尾插法）		.
### **栈（stack）**：
 + (递归的实现用栈保存每次数据的地址) 
```c
 typedef struct{
	ElemType date[];//数据域，ElemType为不确定的数据类型，使用时用typedef从定义即可 ，eg：typedef int ElemType,即可将结构体中的类型定义为int型。
	int top;//用top做栈指针 
}stack; 
```

+  顺序栈（利用数组存储数据，压入，弹出）：
+ 入栈：先检查栈不满则入栈，否则输出-1。
+ 链栈：需要多个栈时（用头插法建立链表，将新节点的地址保存在top栈指针中）
+ 可以用链表实现，每读入一个数据申请一个节点，将新的节点插入到链表的头部，变成第一个节点。
### 递归转换：
+ 迭代：只有一层的递归易转换为迭代（如求n！先求1！再迭代到2！直到n！）
+  尾递归：递归操作仅仅在最后一步进行递归操作，部分编译器可将其优化为空间复杂度与迭代相当。
+  尾递归消除：尾递归是递归调用语句是最后函数最后一句，消除方法用一个变量保存上一次上一次的值。
+ 用栈消除递归
+ Eg：void Outputl(LinkedList L)//顺序输出单链表结点数据的递归算法
```c
{
if(L) {printf(L->data);//输出结点的值、 
Output1(L->next);//尾递归调用

}//0utput1 
Eg：void Output2(LinkedList L)//顺序输出单链表结点数据的非递归算法
{
p=L;//设局部变量p=L
Lbl;//在第一个可执行语句前设标号
if(p){printf(p->data);//输出结点的值
p=p->next;//修改变量值
goto Lb1;//转到第一个可执行语句}
```
 
### 队列
1.	定义：只能在一端插入和删除，通常要两个指针分别指向队头和队尾。
2.	每加入一个元素队列都会整体下移，导致虽然不满但是已经溢出的假溢出现象，可以用循环队列解决，循环队列要设置标志表示队列是否非空。
也可用链表实现，链表的尾指针指向队尾，头指针指向队头。
## 串（string）
1.	即字符串，类似线性表，可采用连续储存也可使用堆或栈储存。
### KMP算法
+	KMP算法是求模式串在主串中第i个字符开始的最长匹配前缀的算法。
+ KMP算法是利用匹配失败后的信息，尽量减少匹配次数，即最大相同前后缀部分的匹配直接跳过。**利用next数组决定可直接跳过前next[i]个数开始匹配**
+ 求next数组：
  - 前缀：如ABC前缀为A，AB，不包含末尾字符。
  - 后缀：如ABC后缀为BC，C，不包含首字符。
  - 求next数组：
    - next[0] = -1，next[i]=max(模式子串最大前后缀相等字符数)
    - 判断当前字符与str[next[i]]是否相等，若相等则next[i]=next[next[i]]否则next[i]不变。（**若跳过后任然匹配失败，则再跳一次**） 


 

## 数组与广义表
1.	一维数组可以看做一个线性表，二维数组可以看做元素为一维数组的线性表。（多维数组可转化为一维数组，eg：a[m][n]中的a[2][3]=3*n+2*1类似为不规则进制的数转化为10进制的数）
2.	矩阵的压缩存储：
+	对称与三角矩阵：存储一半以行为主序的有序对。
+	对角矩阵：以对角线为顺序储存。
+	稀疏矩阵：三元组表（用struct定义int  row(行坐标),col（列）; ElemType e;）
+	利用三元组表可以实现快速转置与计算，交换row与col值输出即可
+	储存稀疏矩阵可以十字链表（即含有四个指针分别指向上下左右）
3.	广式表：
+	简称列表，是线性表的推广。其含有原子与子表。 

 
## 链表：
+ 带头结点的链表：头结点保存第一个数据的地址，不保存数据。**P指向head节点即p->next==head**
+ 循环链表：从任意一个节点都可以历遍整个链表，可通过头指针==尾指针判断是否到达表尾。（通常只设尾指针即可）
+ 双向链表：单链表加入一个指向前驱的指针，可以方便元素的插入与删除。（p->p1->next可以得到p指向的下一个节点所指向的指针地址）
+ 静态链表：
```c  
typedef struct{
				int date;//数据域 
				int next;//指针域 
}startline;建立数组用指针域保存下标。
```
 
## 字典
+ 定义：n个关键字（元素）的集合，每个元素k有一个对应的值。
+ 负载因子：a = d/n，d为字典中元素个数，n为字典中元素个数。
+ 散列表上的查找：建立一个关键词对应地址的函数，该函数就称为散列（哈希）函数。
  +	散列函数建立可能出现冲突（不同关键词得到了同一地址），一般冲突可减少不可避免故应该预设解决冲突的方法。（好的散列函数应该便于计算，冲突少，有解决冲突的方法）
### 散列函数的构造方法：
  （联用下面几种方法，并将取余法作为最后一个，保证关键字在区间中）
+	直接构造，即取关键字为散列函数值。（H(key)=a*key+b）
+	数字分析法：取关键字中重复较少的数位为关键字。
+	 折叠法：将较长的关键字分割为数位相同的几段，相加后舍去进位得到的地址为散列地址，适用关键字长&&每一位数字分布均匀。
+	平方取中法：将关键字平方再取中间的几位数作为关键数，因为平方后的中间几位数与所有的数都有关系》
+	除留余数法：将关键字对合适的数（通常为小于散列函数长度的最大的质数）取余的结果作为关键字。
### 构造散列函数注意：
+	计算散列函数所需的时间。
+	关键字的长度。
+	散列表的长度。
+	关键字的分布情况。
+ 记录的查找频率 
### 散列函数冲突解决方法：
+ 开放地址法：发生冲突时在冲突位置的前后探测空位置存放，为防止冲突二次聚集，通常将步长加到足够长。
+ 线性探查法：从冲突位置的下一个位置开始，依次检查第m、2m、等等，直到找到空位置。
+ 双散列法：利用第二个哈希函数计算m的值，再进行线性探查。
+ 链地址法：用链表保存散列地址，每有一个重复的数就保存在一个新申请的空间中并将用头插反法插入在重复地址之前。

## 树
+ 定义：n个结点的有限集，当n=0时为空树，在任意一棵非空树中：
+ 兄弟：**具有相同父结点的各结点互称为兄弟。**
+ 遍历：
  - 深度优先遍历：
    - 先根遍历：先访问根结点，再访问根的子树。
    - 后根遍历：后访问根结点，再访问根的子树。
  - 广度优先遍历：
    - 按层次遍历。
### 最佳二叉数的搜索树
+ 构造：排序，进行搜索将搜索中遇到的每一个数依次加入树中。
### B树（二叉排序树）
+ m阶二叉排序树定义：n个结点的有限集，若满足：
  - 根节点若有子节点至少有两个子节点。
  - 每个节点的子节点数在[m/2,m]
  - 所有叶子节点在同一层。
### B+树（多路平衡查找树，中间节点本身也保存了信息可供查询）
+ 定义：m阶B+树定义：n个结点的有限集，若满足：
  - 根节点若有子节点至少有两个子节点。
  - 每个节点的子节点数在[m/2,m]
  - 所有叶子节点在同一层。
  - 所有叶子节点包含一个关键字和指向包含这些关键字记录的指针。

### 平衡二叉排序树（AVL树）
+ 定义；
  + 左子树和右子树的高度差绝对值不超过1。
  + 左右子树都是一棵平衡二叉树。
### 已知中序遍历和任意一序遍历求树
+ 中序分开左右子树，前（后）序遍历确定左右子树根结点。
+ 左（右）根节点继续划分左（右）子树。
### 二叉树树与森林的相互转换：
  - 树转二叉树：
    - 所有的根节点互为兄弟，将所有的兄弟节点相互连接。（**注意是否为兄弟节点，相邻节点不一定为兄弟节点。**）
    - 所有父节点只保留与长子的连接，与其他子节点的连接断开。（**注意兄弟不是儿子，特别是根节点互为兄弟不是父子。**）
  - 森林转二叉树：
    - 把所有具有的左子节点的节点A，将其左子节点B的右子节点C及右子节点的右子节点D等等，BD与A连接起来。
    - 去除所有节点与右子节点的连线。
### 堆
+ 定义：完全二叉树，仅有最大堆和最小堆。
+ 堆的存储：
  - 顺序存储：以数组存储，下标为0的元素为根节点，下标为i的元素为i节点的左孩子，下标为2i节点的右孩子，下标为2i+1的左孩子。
  - 链式存储：以链表存储，下标为0的元素为根节点，下标为i的元素为i节点的左孩子，下标为2i节点的右孩子，下标为2i+1的左孩子。
  
+ 堆的建立：以链式存储为例，从最后一个非叶子节点开始，从右到左，从下到上，依次调整堆。
  - 堆的调整：从第一个非终端节点调整，从左到右，从上到下，依次调整堆。
+ 堆排序步骤：
  - 将待排序的序列构造成一个大根堆。
  - 将堆顶元素与大堆最后一个元素交换，将剩余的元素重新构造成一个大根堆。
  - 重复步骤2，直到剩余元素个数为1
2.	应用：数据的压缩，对文章中的字母按频率建立哈曼夫树，重新编码
+ 哈夫曼树：
 - 建立：左子树总小于右子树，且左右子树之和等于父节点。
 - 哈夫曼编码：左分支为1，右分支为0。
 - WPL:计算权重*路径长度之和。

 
## 图

### 基本概念
+ 完全：任意一点与任意一点之间存在路径，无向图边数为n*(n-1)/2，有向图边数为n*(n-1)
+ 连通：任意两点之间存在路径。
+ 强连通：任意两点之间存在路径，且路径长度为1。
+ 连通图：任意两点之间存在路径。
+ 强连通图：任意两点之间存在路径，且路径长度为1。
+ 子图：由原图的子集所构成的图。
+ 连通分量：最大连通子图。
+ 简单路径：点各异边各异，若起点与终点相同为回路。
+ 网络：带权连通图。

### 图的储存
+ 邻接矩阵：
 - 二维数组，存储方式为：
   - 顶点i的邻接点为A[i][j]
   - 顶点i与j之间存在边，A[i][j]=1，否则A[i][j]=0 
   - 行和为：出度
   - 列和为：入度        
 - 邻接表：
   - 一个保存指针的数组，数组中每个元素保存一个链表，链表中保存顶点i的邻接点。

### 最小生成树（MST）
+ 定义：权值之和最小的生成树。
+ 算法：Prim算法，Kruskal算法。
  - prim算法：任取一点加入集合U中的点与外部相连节点中边最小的点加入集合U，直到U中包含所有点。
  - Kruskal算法：依次从边集中选取最小边，若加入后不形成回路，则将边加入生成树，否则舍弃。

### AOE网络
+ 定义：活动以边的形式表示，边表示活动之间的依赖关系。
+ 关键路径：所有活动中最长的。

### 最短路径
+ Dijkstra算法：
  - 思想：Dijkstra算法是一种用于计算加权图（有向图）最短路径的算法。它是一种贪心算法，通过每次找到当前未被访问过的节点中距离起点最近的节点，然后将其加入已访问节点集合，并更新其他节点到起点的距离。最终，算法将找到从起点到终点的最短路径。
  - Dijkstra算法的基本思路如下：
    1. 选择一个未被访问过的节点，将其作为当前节点，并将其加入已访问节点集合。
    2. 更新其他节点到当前节点的距离。具体方法是计算其他节点到当前节点的距离，将其与当前节点到起点的距离相加，得到新的距离。
    3. 如果新的距离小于当前节点到起点的距离，则更新该节点的距离。
    4. 重复步骤2和步骤3，直到所有节点都被访问过为止。
  - Dijkstra算法的实现需要维护一个距离数组，用于存储每个节点到起点的距离。还需要维护一个已访问节点集合，用于记录已经访问过的节点。此外，还需要维护一个邻接表，用于存储每个节点的邻接节点以及它们之间的边权。
  - Dijkstra算法的复杂度为O(|V|^2)，其中|V|表示节点数。因此，对于大规模的图，该算法可能会比较慢。但是，对于小规模的问题，Dijkstra算法是一种简单、高效的方法来计算最短路径。

+ Ford算法：
  - Ford算法是一种用于寻找无向加权图（有向图）单源最短路径的算法。它是一种贪心算法，通过每次找到当前未被访问过的节点中距离起点最近的节点，然后将其加入已访问节点集合，并更新其他节点到起点的距离。最终，算法将找到从起点到终点的最短路径。
  - Ford算法的基本思路如下：
    1. 初始化距离数组和已访问节点集合。
    2. 遍历所有节点，对于每个节点，将其作为当前节点，并将其加入已访问节点集合。
    3. 更新其他节点到当前节点的距离。具体方法是计算其他节点到当前节点的距离，将其与当前节点到起点的距离相加，得到新的距离。
    4. 如果新的距离小于当前节点到起点的距离，则更新该节点的距离。
    5. 重复步骤3和步骤4，直到所有节点都被访问过为止。
  - Ford算法的实现需要维护一个距离数组，用于存储每个节点到起点的距离。还需要维护一个已访问节点集合，用于记录已经访问过的节点。此外，还需要维护一个邻接表，用于存储每个节点的邻接节点以及它们之间的边权。
  - Ford算法的复杂度为O(|V|^2)，其中|V|表示节点数。因此，对于大规模的图，该算法可能会比较慢。但是，对于小规模的问题，Ford算法是一种简单、高效的方法来计算最短路径。



# 机器学习
+ 定义：计算机程序针对特定任务，通过经验学习自动改进性能。
+ 机器学习是人工智能的基础，深度学习是机器学习的一种。
+ 分类：
  - 监督学习：有标签：对于输入数据X可预测Y
    eg：回归、K临近、SVM、决策树、朴素贝叶斯、逻辑回归
  - 无监督学习：无标签：对于输入数据X能发现什么
    eg：聚类、PCA（降维算法）、EM算法
  - 强化学习：有奖励机制，序列决策问题。
    eg:马尔科夫决策算法
+ 开发一般步骤：
  - 数据采集和标记：采集训练样本，并对其进行标记。
  - 数据清洗：数据进行预处理，使得模型可直接使用。
  - 特征选择
  - 模型选择
  - 模型训练和测试
  - 模型性能评估和优化
## 基础概念
+ 过拟合:是指模型能很好地拟合训练样本 ，但对新数据的预测准确性很差 。
  - 减少输入特征数量。
  - 获取更多训练数据。
  - 正则化：在损失函数中添加正则项，使模型参数变小，从而降低过拟合的风险。
+ 欠拟合:是指模型不能很好地拟合训练样本 ，且对新数据的预测准确性也不好
  - 增加输入特征数量。
  - **为什么增加多项式特征可以优线性回归模型准确性**？答：通过增加多项式特征，实际上是增加了模型的复杂度。许多实际数据的分布是复杂的、非线性的。增加多项式特征可以让模型更好地适应这种复杂的数据分布
  - 增加特征项。
+ 成本 :是针对所有训练样本 ，模型拟合出的值与训练样本的真实值的 误差平均值 。成本是衡量模型与训练样本符合程度的指标 。
+ 成本函数: 是成本与模型参数的函数关系
+ 模型准确性：成本函数越小，模型准确性越高。(按8:2/7:3划分训练集和测试集，
  将数据集划分为训练集和测试集，训练集用于训练模型，测试集用于评估模型准确性。)
  - 交叉验证：数据集划分为训练集、交叉验证集、测试推荐6:2：2,以不同参数在训练集
    上训练，在交叉验证集上评估，选出最优参数，最后在测试集上评估。
+ 学习曲线：以训练集大小为横坐标，以模型准确性为纵坐标，绘制出的曲线。用于观察模型准确性与训练集大小的关系。
+ 查准率: 是模型预测为正例的样本中，实际为正例的比例。
+ 召回率: 是实际为正例的样本中，模型预测为负例的比例。
+ scikit-learn 广播机制：两个矩阵运算，维度不匹配时，将小维度扩展为与另一个矩阵匹配的维度。
## KNN （有监督的机器学习算法）
+ KNN（K-Nearest Neighbors），即K近邻算法，是一种基本的分类和回归算法：
### 基本原理
+ **核心思想**：对于一个新的数据点，在训练数据集中找到与它最相似的K个邻居，然后根据这K个邻居的类别来预测新数据点的类别，或根据数值预测数值。
+ **距离度量**：通常使用欧氏距离、曼哈顿距离、闵可夫斯基距离等来衡量数据点之间的相似性。例如，欧氏距离的计算公式为：\(d(x,y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}\)，其中\(x=(x_1,x_2,\cdots,x_n)\)和\(y=(y_1,y_2,\cdots,y_n)\)是两个数据点，\(n\)是数据的维度。
+ **K值**：
  -  K值越小，模型方差越大可能导致模型过拟合，
  -  K值越大，模型偏差越大,可能使模型过于平滑而欠拟合。
+ **成本函数**：
### 特点
- **优点**：
    - 高准确性，对异常值和噪声容忍高。
- **缺点**：
    - 计算量大，依赖内存。
### 应用场景
- **分类问题**：如手写数字识别、图像分类、文本分类等。在手写数字识别中，将手写数字的图像转换为特征向量，然后使用KNN算法根据训练集中的已知数字图像来识别新的手写数字图像所属的类别。
- **回归问题**：如房价预测、股票价格预测等。在房价预测中，根据房屋的面积、位置、房龄等特征，使用KNN算法预测新房屋的价格。
- **推荐系统**：可以根据用户的历史行为数据，如购买记录、浏览记录等，找到与目标用户最相似的K个用户，然后根据这些相似用户的喜好来为目标用户推荐商品或服务。
### **KNN 变种**
+ 为每一个临近点赋予一个权重，权重与距离成反比。
+ 使用在半径R内的所有点取代距离最近的K个点。
### 聚类算法性能评估
+ 齐次性/完整性：同一类型数据是否在同一类中。
+ 轮廓系数（可评估无监督学习）：($s=\frac{b-a}{max(a,b)}$),其中a为样本i到同类中其他样本点的平均距离，b为样本i到距离最近下一个类中样本点的平均距离。
### 实现步骤
- **数据准备**：收集和整理训练数据集，包括特征向量和对应的标签（分类任务）或目标值（回归任务）。
- **确定K值**：选择合适的K值，一般通过交叉验证等方法来确定。K值的选择会影响模型的性能，较小的K值可能导致模型过拟合，较大的K值可能使模型过于平滑而欠拟合。
- **计算距离**：对于待预测的数据点，计算它与训练数据集中每个数据点的距离。
- **选择邻居**：根据计算出的距离，找出距离最近的K个邻居。
- **预测结果**：
- **分类任务**：根据这K个邻居中多数的类别来决定待预测数据点的类别，通常采用投票法，即选择出现次数最多的类别作为预测结果。
- **回归任务**：计算这K个邻居的目标值的平均值或加权平均值作为待预测数据点的预测值，权重可以根据距离的远近进行设置，距离越近权重越大。
### **k-均值算法与k-近邻算法的区别**
+	目的不同
  - k均值算法：是一种无监督学习算法，主要用于聚类分析，将数据划分为不同的簇，发现数据的内在分布结构，
    使得同一簇内的数据点尽可能相似，不同簇之间的数据点尽可能不同，簇的划分是根据数据点到簇中心的距离。
  - k近邻算法：是一种监督学习算法，主要用于分类和回归任务（通常用于分类）。
    它根据训练数据集中与新样本最近的个邻居的类别来预测新样本的类别（分类时）
    或通过邻居的数值来估计新样本的数值（回归时）。
+	算法过程不同
  - k均值算法：重点在于不断更新簇中心和重新分配数据点到簇中，
    通过迭代优化来确定最终的个簇。在计算距离时，是计算数据点到簇中心的距离。
  - k - 近邻算法：在预测时，计算新样本到训练数据集中所有样本的距离，找到最近的个邻居，
    然后根据这个邻居的信息（如多数邻居的类别等）来做出预测。
    在训练阶段，k - 近邻算法只是简单地存储训练数据，没有复杂的训练过程。
+ 输出结果不同
- k - 均值算法：输出是个簇的划分，以及每个簇的质心等信息，它描述了数据的聚类结构。
- k - 近邻算法：输出是对新样本的预测类别（分类任务）或预测数值（回归任务），是一个具体的预测值。


## 线性回归算法
+ 是一种广泛应用于机器学习领域的监督学习算法，用于建立自变量和因变量之间的线性关系模型，以下是对其详细的介绍：
### 基本原理
+ 线性回归基于这样一个假设：自变量 \(x\) 和因变量 \(h(\theta)\) 之间存在线性关系，其**预测函数**为 \(h(\theta) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n + \theta\)，其中 \(h(\theta)\) 是因变量，\(x_1,x_2,\cdots,x_n\) 是自变量，\(\theta_0,\theta_1,\cdots,\theta_n\) 是模型的参数。
  - **预测函数矩阵形式**：\(h(\theta) = [\theta_0,\theta_1,\cdots,\theta_n] \cdot [x_0,x_1,\cdots,x_n]^T=X\theta^T\)，其中 \(X\) 是自变量矩阵转置，\(\theta\) 是参数向量矩阵，\(\theta\) 是误差向量。
+ 成本函数(衡量预测效果)：$j(\theta)=\frac{1}{2m}\sum_{i=1}^{m} (h(x_i)-y_i)^2$，矩阵形式：$J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)$,矩阵形式无需累加器，矩阵运算效率高。
+ 梯度下降的公式为 \(\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}\) ，其中 \(\alpha\) 是学习率，\(J(\theta)\) 是目标函数，\(\frac{\partial J(\theta)}{\partial \theta_j}\) 是目标函数对参数 \(\theta_j\) 的偏导数。 **\(\alpha\) 学习率：过大可能越过极小值点，模型无法收敛；过小可能导致收敛速度过慢。**
+ 梯度下降的参数迭代公式：将成本函数代入梯度下降的公式结果为：$$\theta_j := \theta_j - \frac{\alpha }{m} \sum_{i=1}^{m} (h(x_i)-y_i)x_j$$
### 附加操作
+ 数据归一化：将数据缩放到一个较小的范围内，如[0,1]或[-1,1]，可提高计算效率，加快收敛速度，但**对模型准确性无影响**
+ 增加特征项：实际数据为非线性数据，通过增加特征项，提高拟合效果。
+ 梯度下载算法目的：通过迭代求得更佳的$\theta$，使成本函数最小化,$\theta$移动的方向由偏导数决定。
### 优缺点
+ 线性回归假设自变量和因变量之间存在线性关系，如果实际关系是非线性的，线性回归模型的拟合效果可能会很差。
+ 对异常值比较敏感，异常值可能会对模型的参数估计和预测结果产生较大的影响。
+ 可能存在多重共线性问题，即自变量之间存在高度相关性，这可能会导致模型的参数估计不稳定，影响模型的预测性能。

## 逻辑回归算法（解决分类问题）
### 基本原理
+ 逻辑回归主要用于分类问题，它假设输入特征 \(x=(x_1,x_2,\cdots,x_n)\) 与样本属于某一类别的概率 \(p(y=1|x)\) 之间存在一种映射关系。其预测函数通过引入一个非线性的激活函数（通常为Sigmoid函数）来构建。
+ 预测函数表示为 \(h_{\theta}(x) = \frac{1}{1 + e^{-\theta_0 - \theta_1x_1 - \theta_2x_2 - \cdots - \theta_nx_n}}\)，这里\(\theta = (\theta_0,\theta_1,\cdots,\theta_n)\) 是模型的参数，\(h_{\theta}(x)\) 可以解释为给定输入特征 \(x\) 时，样本属于正类（比如 \(y = 1\) 所代表的类别）的概率，即 \(P(y = 1|x)=h_{\theta}(x)\)，那么样本属于负类（\(y = 0\) ）的概率就是 \(P(y = 0|x)=1 - h_{\theta}(x)\)。
+ **预测函数矩阵形式**：\(h_{\theta}(x) = \frac{1}{1 + e^{-\theta^TX}}\)

+ 逻辑回归成本函数（衡量预测效果）：\(J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y_i\log(h_{\theta}(x_i))+(1 - y_i)\log(1 - h_{\theta}(x_i))]\)
  - 这里 \(m\) 是训练样本的数量，其目的是衡量模型预测概率与真实类别标签之间的差异，通过最小化这个成本函数来训练模型找到最优的参数 \(\theta\) 。
  - 逻辑回归常用的成本函数是对数似然损失函数，对于单个样本 \((x_i,y_i)\) ，其成本函数为：
  \[
  \begin{cases}
  -\log(h_{\theta}(x_i)) & \text{if } y_i = 1 \\
  -\log(1 - h_{\theta}(x_i)) & \text{if } y_i = 0
  \end{cases}
  \]
  - **与线性回归成本函数对比**：不同于线性回归的成本函数 \(j(\theta)=\frac{1}{2m}\sum_{i=1}^{m} (h(x_i)-y_i)^2\) ，逻辑回归的成本函数基于概率和对数似然的概念构建，更符合分类问题中对概率估计准确性的衡量需求，而线性回归的成本函数侧重于衡量预测值与真实值差值的平方和。
+ 梯度下降
  - 同样采用梯度下降算法：\(\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}\) ，其中 \(\alpha\) 是学习率，\(J(\theta)\) 是成本函数，\(\frac{\partial J(\theta)}{\partial \theta_j}\) 是成本函数对参数 \(\theta_j\) 的偏导数。
+ **梯度下降的参数迭代公式推导（以逻辑回归为例）**：经过一系列的求导运算（基于链式法则等对成本函数 \(J(\theta)\) 求关于 \(\theta_j\) 的偏导数），将成本函数代入梯度下降的公式结果为：
  \[
  \theta_j := \theta_j - \frac{\alpha }{m} \sum_{i=1}^{m} (h_{\theta}(x^i)-y^i)x_j^{(i)}
  \]
+ 过拟合解决
  - 正则化：解决模型过拟合，保留所有的特征减少特征权重，
    操作为预测函数中加入正则项，正则参数($\lambda$)越小拟合效果越好
  - 减少输入特征
  - 获取更多训练数据
+ 正则项：$\frac{1}{2m} \sum_{i=1}^{m} \theta^2_k$,再逻辑回归模型加上正则项即完成模型正则化。正则化后模型的梯度下降的参数迭代公式：$\theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \frac{\alpha }{m} \sum_{i=1}^{m} (h_{\theta}(x^i)-y^i)x_j^{(i)}$，其中$\lambda$为正则化系数，$\lambda$越大，正则化越强，$\lambda$越小，正则化越弱。
+ **L1|L2正则项**：L1范数：$||\theta||_1=\sum_{i=1}^{n}|\theta_i|$，L2范数：$||\theta||_2 = \sqrt{\sum_{i=1}^{n}\theta_i^2}$
+ 最大似然估计: 通过现实世界统计概率，来估计模型参数，使模型预测结果与实际结果最接近。（eg：理论硬币概率0.5:0.5,但在N次统计时概率为0.3:0.7出现的概率最大，则模型参数选择为0.3:0.7）
# 应用场景与问题
+ 乳腺癌预测
+ 怎样理解正则化后的成本函数解决了过拟合问题？
  - 从数学角度看，增加了正则项$\frac{\lambda}{2m}\sum_{k = 1}^{n}\theta_{k}^{2}$后，成本函数不再唯一地由预测值与真实值的误差所决定，还和参数$\theta$的大小有关。
  - 有了这个限制之后，要实现成本函数最小的目的，$\theta$就不能随便取值了。
  - 比如某个比较大的$\theta$值可能会让预测值与真实值的误差$(h_{\theta}(x)-y)^{2}$值很小，但会导致$\theta$很大，最终的结果是成本函数太大。这样，通过调节参数$\lambda$，就可以控制正则项的权重，从而避免线性回归算法过拟合。

## 决策树算法（解决分类问题）
### 基本原理
+ 决策树是一种树形结构，每个分支节点代表一个特征判断，根据判断结果进行分类，每个叶节点代表一种类别。
+ 信息熵：香农提出信息熵（信息不确定性越高信息熵越高），解决信息量化问题。**信息熵公式**：$H(x)=-\sum_{x\subset X}^{}P(x)log_2P(x)$，其中P(X)为X发生的概率，单位比特
+ 信息增益：信息增益是用来衡量数据变得更有序、更纯净的程度的指标
+ 构建决策树时，应该优先选择哪个特征来划分数据集答案是：遍历所有的特征，分别计算，得到每个特征划分数据集前后信息的变化值，选择信息熵变化幅度最大的那个特征，优先作为数据集划分依据。即选择**信息增益**最大的特征作为分裂节点。
+ 构建结束标志：
  - 可用特征集为空
  - 继续划分提升信息增益小于设定值。
### 决策树创建步骤：
+ 计算数据集划分前的信息熵；
+ 遍历所有未作为划分条件的特征，分别计算根据每个特征划分数据集后的信息熵；
+ 选择信息增益最大的特征，并使用这个特征作为数据划分节点划分数据；
+ 递归地处理被划分后的所有子数据集，从未被选择的特征里继续选择最优数据划分特征来划分子数据集。
### 决策树相关问题解决
+ 连续属性值
  - 将连续属性值划分为多个区间，每个区间取一个值，然后按照离散属性值的方法进行划分。
+ 优先选择取值较多的特征：
  - 计算信息熵时加上一个与类别个数成正比的正则项。
### ID3算法
+ ID3算法使用信息增益作为特征选择的标准，选择信息增益最大的特征作为分裂节点，递归地构建决策树。但倾向于选择取值较多的特征，可能导致过拟合。
+ **解决优先选择取值较多的特征**：计算信息熵时加上一个与类别个数成正比的正则项。
### CART 算法：
+ 采用基尼指数作为特征选择的标准，可用于分类和回归任务。在分类任务中，基尼指数越小，说明该特征对分类的纯度提升越大；在回归任务中，通过最小化平方误差来选择最优的划分特征和划分点。CART 算法生成的决策树是二叉树，结构相对简单，易于理解和实现。
+ **解决部分属性值连续**：将其划分为多个区间，每个区间取一个值。实现离散化。
+ 基尼指数：衡量信息不纯度指标，越接近0纯度越高。**基尼指数公式**：$Gini(D)=1-\sum_{k=1}^{K}p_k^2$，其中K为类别个数，$P_k$为样本属于第k类的概率。
### 决策树过拟合解决：
+ 前剪枝：在决策树构建过程中，通过设置最大深度、最小样本数、最大叶节点数等参数来限制树的深度，从而避免过拟合。
+ 后剪枝：在决策树构建完成后，通过比较子分支是否有继续分裂的必要来删除一些不必要的分支，从而简化决策树的结构，提高模型的泛化能力。


## 朴素贝叶斯算法（解决分类问题）
### 基本原理
- **贝叶斯定理**：在分类问题中，假设我们有类别\(Y\)和特征向量\(X=(X_1,X_2,\cdots,X_n)\)，贝叶斯定理表示为\(P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}\)。
- **“朴素”**：朴素贝叶斯算法**假设特征\(X_1,X_2,\cdots,X_n\)在给定类别\(Y\)的条件下是相互独立的**。这意味着\(P(X|Y)=P(X_1|Y)P(X_2|Y)\cdots P(X_n|Y)\)。基于这个假设，朴素贝叶斯分类器可以简化计算，大大提高了计算效率。
- 在朴素贝叶斯中，假设特征之间相互独立（这是一个较强的假设），则P(x|y)=P(x1|y)*P(x2|y)…*P(xi|y)。
  通过训练数据估计出P(y)（各类别的先验概率）和xi（每个特征在各类别下的条件概率）
  然后对于新样本x，计算每个类别对应的P(x,|y)P(y)，取其最大值对应的类别作为预测类别。
### 工作流程
+ **准备数据**：收集和整理带有类别标签的训练数据，每个样本包含一组特征和对应的类别。
+ **计算先验概率**：对于每个类别\(C\)，计算其在训练数据中出现的概率\(P(C)\)，即类别\(C\)的样本数除以总样本数。
+ **计算类条件概率**：对于每个特征\(X_i\)和类别\(C\)，计算在类别\(C\)中特征\(X_i\)出现的概率\(P(X_i|C)\)。通常可以通过统计在类别\(C\)的样本中特征\(X_i\)出现的频率来估计。
+ **预测分类**：对于一个新的样本\(X=(X_1,X_2,\cdots,X_n)\)，计算它属于每个类别\(C\)的后验概率\(P(C|X)\)。根据贝叶斯定理和特征条件独立假设，\(P(C|X)=\frac{P(X|C)P(C)}{P(X)}=\frac{P(X_1|C)P(X_2|C)\cdots P(X_n|C)P(C)}{P(X)}\)。由于\(P(X)\)对于所有类别都是相同的，所以在比较不同类别的后验概率时可以忽略不计。因此，只需要计算\(P(X_1|C)P(X_2|C)\cdots P(X_n|C)P(C)\)，并选择概率最大的类别作为预测结果。

### 优点
+ **算法简单高效**：基于贝叶斯定理和特征独立假设，计算过程相对简单，易于理解和实现，在大规模数据集上训练和预测速度快。
+ **对小规模数据表现良好**：在数据量较小的情况下，朴素贝叶斯仍然能够取得较好的分类效果，尤其适用于文本分类、垃圾邮件过滤等领域。
+ **对缺失数据不敏感**：在实际应用中，数据常常存在缺失值，朴素贝叶斯算法在处理缺失数据时相对简单，不需要对缺失值进行复杂的填充或处理。

### 缺点
+ **特征独立性假设限制**：在实际应用中，特征之间往往存在一定的相关性，这可能导致朴素贝叶斯算法的分类性能下降。
+ **对输入数据的表达形式敏感**：在文本分类中，如果文本的表示方式发生变化，例如词袋模型中的词汇选择、词频统计方法等，可能会对分类结果产生较大影响。

### 应用场景
+ **文本分类**：如新闻分类、情感分析、垃圾邮件过滤等，将文本划分为不同的类别，如政治、经济、娱乐等，或判断文本的情感倾向是积极、消极还是中性，以及识别垃圾邮件和正常邮件。
+ **疾病诊断**：根据患者的症状、检查结果等特征，判断患者可能患有的疾病。例如，根据咳嗽、发热、流涕等症状，判断患者是感冒、流感还是其他疾病。
+ **推荐系统**：基于用户的历史行为和偏好，预测用户对不同物品的喜好程度，从而进行个性化推荐。例如，根据用户对电影的评分、观看历史等，推荐其他可能感兴趣的电影。

##  PCA 算法（主成分分析算法）：
+ PCA 是一种常用的数据降维算法。
  其主要思想是将高维数据投影到低维空间，同时尽可能地保留数据的方差信息（即数据的分布特征）。
  通过计算数据的协方差矩阵，求其特征值和特征向量，
  选择较大特征值对应的特征向量组成投影矩阵，将原始数据投影到新的低维空间。
###	PCA算法作用
+	数据可视化：当数据维度很高时，无法直接在二维或三维空间中可视化。
  PCA 可以将高维数据降到二维或三维，从而可以绘制数据点，直观地观察数据的分布模式
  、聚类情况等。例如在分析一个有多个特征（如年龄、收入、消费习惯等多个维度）
  的客户数据集时，通过 PCA 降到二维或三维后在平面或空间中绘制客户点，观察客户群体的分布特征。
+	降低计算复杂度：在很多机器学习算法中，计算复杂度随着数据维度的增加而急剧增加。
  PCA 降维后，数据维度降低，在后续的数据分析、模型训练等过程中可以减少计算量和存储空间。
  例如在处理图像数据（图像像素点多，维度高）时，先进行 PCA 降维再进行分类或聚类等操作，可以提高计算效率。
+	去除噪声和冗余信息：高维数据中可能存在一些冗余特征或噪声信息，
  PCA 在降维过程中，通过保留主要成分（方差较大的方向），
  可以在一定程度上过滤掉这些噪声和冗余信息，提高数据质量，
  使得后续基于降维后数据的分析和模型更加准确和稳定。
### PCA算法的数据还原率
+ 数据还原率表示通过 PCA 降维后的数据再还原回原始空间时，能够恢复原始数据信息的程度。
  反映了 PCA 算法在降维过程中对原始数据信息的保留情况，
  如果还原率高，说明降维过程中丢失的信息较少；
  反之，如果还原率低，则丢失的信息较多。
+ 数据还原率计算方法：
  - **计算原理**：在PCA中，首先将原始数据进行中心化处理，然后通过计算协方差矩阵的特征值和特征向量，选择前\(k\)个最大特征值对应的特征向量构成投影矩阵，将原始数据投影到低维空间。
    在还原数据时，将低维数据通过投影矩阵的逆变换或伪逆变换映射回原始高维空间，得到重构数据。数据还原率可以通过计算原始数据与重构数据之间的误差来衡量，误差越小，还原率越高。
  - **计算公式**：假设原始数据矩阵为\(X\)，经过PCA降维后再还原得到的重构数据矩阵为\(\hat{X}\)，则重构误差\(E\)可以用均方误差（MSE）来表示，
    计算公式为\(E = \frac{\frac{1}{m}\sum_{i = 1}^{m}||x_i - \hat{x}_i||^2}{\frac{1}{m}\sum_{i = 1}^{m}||x_i||^2}\)，其中\(n\)是数据点的数量，\(x_i\)是原始数据中的第\(i\)个数据点，\(\hat{x}_i\)是重构数据中的第\i个数据点。数据还原率\(R\)可以定义为\(R = 1 - \frac{E}{E_{max}}\)，其中\(E_{max}\)是当重构数据为随机噪声或零矩阵时的最大可能误差，通常可以用原始数据的方差来近似表示，即\(E_{max}=\frac{1}{n}\sum_{i = 1}^{n}||x_i - \bar{x}||^2\)，其中\(\bar{x}\)是原始数据的均值向量。

## 支持向量机（SVM）：二元分类问题
+ SVM：按监督学习方式对数据进行二元分类的广义分类器，
  其决策边界是对样本学习的求解的最大边距平面。
+ SVM 原理：找到一个分割平面，其可正确分类且间距最大。（类似一条线分割红黑双方多个点）
+ （软）硬间隔：（不）允许分类出现个别分类错误。
+ SVM 超参：C（惩罚系数，控制分类错误的惩罚程度，C 越大，对分类错误的惩罚越重，拟合越高）。$\gamma$（$\gamma$ 越大，支持向量越少，分类边界越接近一次函数，可能过拟合）

## 感知机
+ 定义：当输入多个信号（$x_1,...,x_n$）,信号乘上固定的权重（$w_1,...,w_n$）并求和，然后加上一个偏置（$b$）传入$h(x)$得到输出（$y,x>0->1,x<0->1$）。（$y=h(\sum_{i=1}^{n}w_ix_i+b)$）
+ 参数：$b$（偏置），-b称为阈值，反映了神经元的激活难度。
+ $y=h(\sum_{i=1}^{n}w_ix_i+b)$，称为突变函数，超过了一定阈值，输出为1，否则为0。
+ 局限性：不能实现异或门，只能解决线性可分问题。
+ 多层感知机：感知机可以堆叠多层，解决非线性问题。

## 神经网络
+ 定义：由多个神经元（感知机）组成输入层、中间层（隐藏层）、输出层组成的网络。
+ 激活函数（连接感知机和神经网络的桥梁）：$y=h(\sum_{i=1}^{n}w_ix_i+b)$，$h(x)$称为激活函数，用于将神经元的输入信号转换为输出信号，$h(x)=\frac{1}{1+e^{-x}}$，$h(x)=y$输出为0~1间连续的值，称为sigmoid函数，故神经网络中流动的是连续的实数信号。
  - sigmoid为非线性函数，若h(x)为线性函数，则多层神经网络可简化为两层，神经网络为线性模型，无法解决非线性问题。
+ 神经网络中信号传递：信号从输入层传递到输出层，信号经过隐藏层时，信号会经过激活函数（$f(x)$）进行非线性变换，再传递到下一层。
+ 隐藏层可神经元数可多于输入层，但之后的隐藏层神经元数一般比前一层少，做一个特征压缩。
+ 损失函数引入：
  - 不采用识别精度为指标：因识别精度对微小的参数变化不敏感，无法通过梯度下降法优化参数。
  - 不使用跃迁函数为激活函数：因其对于微小的参数变化不敏感，无法通过梯度下降法优化参数。
  - 损失函数，用于衡量模型预测结果与真实结果之间的差异。
+ 交叉熵误差：部分样本同时属于两个类别。
+ 多分类问题：采用softmax：$softmax(y_k)=\frac{e^{x_k}}{\sum_{k=1}^{n}e^{x_k}}$，其中$k$为类别数，$y_k$为输出层第$k$类别的神经元输出值，$x_k$为输出层第K个神经元输入信号，n为输出层神经元个数。
  - $softmax(x_k)$为第$k$类别的输出概率，$softmax(x_k)$的输出值之和为1故可将其输出称为概率，可以用于多分类问题。将输出层神经元的输出值转换为概率分布，使得所有输出值之和为1，可以用于多分类问题。
+ SGD:随机梯度下降算法，通过随机选择数据点，计算梯度，更新参数，迭代优化模型。
+ 反向传播：（正向传播存在大量计算，但已知结果和计算步骤，可反推出计算过程中各个步骤的中间结果，进而得到参数）计算损失函数对每个参数的梯度，用于更新参数，优化模型。










